{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI jokes test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPJTfI7MKNMYCY5x602y9KB"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAbk-dnJ64iz"
      },
      "source": [
        "# **What if an AI could make jokes?**\n",
        "\n",
        "After reading about a guy who created a bot who generated jokes from a library called pyjokes,I wondered if we could create an AI that would be able to write jokes by itself. \n",
        "\n",
        "I started my researches and found about this magic library called [GPT-2](https://openai.com/blog/better-language-models/). \n",
        "\n",
        "The dataset I used can be found here https://github.com/taivop/joke-dataset\n",
        "\n",
        "The results are absurdly hilarious. \n",
        "\n",
        "\n",
        "```\n",
        "Why do people always say the same thing|Because they don't know what to say.\n",
        "```\n",
        "\n",
        "```\n",
        "What do you call a guy who has a little bit of alcohol?|A drunk..\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUjOrHwdJ2we",
        "outputId": "e5db460b-d47e-456d-9405-f7318cdbaefe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Installing dependencies\n",
        "!pip3 install --user tensorflow==1.14"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2MB)\n",
            "\u001b[K     |████████████████████████████████| 109.2MB 99kB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.33.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.12.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.8.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.18.5)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.35.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.1.0)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (3.12.4)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 56.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.3.3)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 52.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.2.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.14) (50.3.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.3.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.4.0)\n",
            "Installing collected packages: keras-applications, tensorboard, tensorflow-estimator, tensorflow\n",
            "\u001b[33m  WARNING: The script tensorboard is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The scripts freeze_graph, saved_model_cli, tensorboard, tf_upgrade_v2, tflite_convert, toco and toco_from_protos are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "Successfully installed keras-applications-1.0.8 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRgKL-DWNIG1",
        "outputId": "b2fb70e5-084a-43f2-fc46-bf2a412d1b61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install keras_applications==1.0.4 --no-deps"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras_applications==1.0.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/90/8f327deaa37a71caddb59b7b4aaa9d4b3e90c0e76f8c2d1572005278ddc5/Keras_Applications-1.0.4-py2.py3-none-any.whl (43kB)\n",
            "\r\u001b[K     |███████▌                        | 10kB 23.9MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20kB 22.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 30kB 14.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 40kB 11.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 5.4MB/s \n",
            "\u001b[?25hInstalling collected packages: keras-applications\n",
            "  Found existing installation: Keras-Applications 1.0.8\n",
            "    Uninstalling Keras-Applications-1.0.8:\n",
            "      Successfully uninstalled Keras-Applications-1.0.8\n",
            "Successfully installed keras-applications-1.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYCE0u4GNjId",
        "outputId": "8d1be747-5d31-417b-b9d9-21a02a9dd4ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install keras_preprocessing==1.0.2 --no-deps"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras_preprocessing==1.0.2\n",
            "  Downloading https://files.pythonhosted.org/packages/71/26/1e778ebd737032749824d5cba7dbd3b0cf9234b87ab5ec79f5f0403ca7e9/Keras_Preprocessing-1.0.2-py2.py3-none-any.whl\n",
            "Installing collected packages: keras-preprocessing\n",
            "  Found existing installation: Keras-Preprocessing 1.1.2\n",
            "    Uninstalling Keras-Preprocessing-1.1.2:\n",
            "      Successfully uninstalled Keras-Preprocessing-1.1.2\n",
            "Successfully installed keras-preprocessing-1.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJ-qVADl6si1",
        "outputId": "1d82d038-d134-435b-c640-faa17a2b914e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/nshepperd/gpt-2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 375, done.\u001b[K\n",
            "remote: Total 375 (delta 0), reused 0 (delta 0), pack-reused 375\u001b[K\n",
            "Receiving objects: 100% (375/375), 4.43 MiB | 22.77 MiB/s, done.\n",
            "Resolving deltas: 100% (204/204), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqikdfVm6y14",
        "outputId": "c2a8a65a-4246-47bb-b465-680ef9fdcad8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cd gpt-2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gpt-2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j68v-Ks4Ip99",
        "outputId": "ad29c7f3-7872-4053-8f11-3ca9675459a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fire>=0.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/a7/0e22e70778aca01a52b9c899d9c145c6396d7b613719cd63db97ffa13f2f/fire-0.3.1.tar.gz (81kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 5.6MB/s \n",
            "\u001b[?25hCollecting regex==2017.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
            "\u001b[K     |████████████████████████████████| 604kB 8.4MB/s \n",
            "\u001b[?25hCollecting requests==2.21.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/e3/20f3d364d6c8e5d2353c72a67778eb189176f08e873c9900e10c0287b84b/requests-2.21.0-py2.py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 10.1MB/s \n",
            "\u001b[?25hCollecting tqdm==4.31.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 9.0MB/s \n",
            "\u001b[?25hCollecting toposort==1.5\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/8a/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4/toposort-1.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2020.6.20)\n",
            "Collecting idna<2.9,>=2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 10.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: fire, regex\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.3.1-py2.py3-none-any.whl size=111005 sha256=58474e92fc9e5b4c78f31c5cca7efaa4732b1cebb06a823499d5e63fbd4e78fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/61/df/768b03527bf006b546dce284eb4249b185669e65afc5fbb2ac\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2017.4.5-cp36-cp36m-linux_x86_64.whl size=533191 sha256=ff9dc1638044c37f08e7da58a8e9bf4867587a5df5a260070688c9414494305f\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
            "Successfully built fire regex\n",
            "\u001b[31mERROR: spacy 2.2.4 has requirement tqdm<5.0.0,>=4.38.0, but you'll have tqdm 4.31.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.21.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement tqdm>=4.36.1, but you'll have tqdm 4.31.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: fire, regex, idna, requests, tqdm, toposort\n",
            "  Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "Successfully installed fire-0.3.1 idna-2.8 regex-2017.4.5 requests-2.21.0 toposort-1.5 tqdm-4.31.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WEzlVgU6_T3",
        "outputId": "8dda201e-563f-40c9-ed80-69ad1cab8fd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#The model I use here is the small one 117M but there is a medium one 355M\n",
        "!python download_model.py 117M"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rFetching checkpoint:   0%|                                              | 0.00/77.0 [00:00<?, ?it/s]\rFetching checkpoint: 1.00kit [00:00, 1.12Mit/s]                                                     \n",
            "\rFetching encoder.json:   0%|                                           | 0.00/1.04M [00:00<?, ?it/s]\rFetching encoder.json: 1.04Mit [00:00, 49.3Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 822kit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:07, 68.5Mit/s]                                  \n",
            "Fetching model.ckpt.index: 6.00kit [00:00, 5.53Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 472kit [00:00, 60.3Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 63.9Mit/s]                                                       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r67SnhDx8bKh",
        "outputId": "68441663-aacf-4ba1-85f0-b76d6fc417df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Extracting and putting the document in the right format to use\n",
        "!curl -O https://raw.githubusercontent.com/taivop/joke-dataset/master/reddit_jokes.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 65.4M  100 65.4M    0     0  65.7M      0 --:--:-- --:--:-- --:--:-- 65.6M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhI5J5V284LL",
        "outputId": "69599d75-2198-4f36-bf80-20bb6838cd43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "jokes_raw = json.loads(Path(\"reddit_jokes.json\").read_text())\n",
        "\n",
        "jokes_parsed = \"<|endoftext|>\".join(\"{0}|{1}\".format(j['title'], j['body']) for j in jokes_raw)\n",
        "\n",
        "Path(\"input-text.txt\").write_text(jokes_parsed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51264267"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCyaC2u49Htp",
        "outputId": "fc4fc1c7-7804-4f9a-df8c-ea528f270620",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install tqdm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.31.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pY5EfPvo9NWN",
        "outputId": "3dadf257-2c91-4d15-d303-1a0514820fd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!PYTHONPATH=src ./encode.py input-text.txt input-text.txt.npz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "Reading files\n",
            "100% 1/1 [00:55<00:00, 55.25s/it]\n",
            "Writing input-text.txt.npz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28MX0Afg9RFU",
        "outputId": "df5c51d8-07b0-48a3-ea63-82e5d16508e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Here we are training the model with a sample every 250 epoch\n",
        "!PYTHONPATH=src ./train.py --dataset input-text.txt.npz --sample_every=250 --save_every=250"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/memory_saving_gradients.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:88: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:91: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2020-11-08 14:21:54.561781: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2020-11-08 14:21:54.565474: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
            "2020-11-08 14:21:54.565655: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3187480 executing computations on platform Host. Devices:\n",
            "2020-11-08 14:21:54.565683: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "WARNING:tensorflow:From ./train.py:92: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:16: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From ./train.py:121: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:144: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:147: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:149: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:152: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "2020-11-08 14:22:06.563250: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
            "Loading checkpoint models/117M/model.ckpt\n",
            "WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Loading dataset...\n",
            "100% 1/1 [00:00<00:00,  2.14it/s]\n",
            "dataset has 13840331 tokens\n",
            "Training...\n",
            "[1 | 22.35] loss=3.06 avg=3.06\n",
            "[2 | 41.97] loss=2.98 avg=3.02\n",
            "[3 | 61.30] loss=3.52 avg=3.19\n",
            "[4 | 80.49] loss=2.80 avg=3.09\n",
            "[5 | 99.78] loss=3.26 avg=3.12\n",
            "[6 | 119.25] loss=2.74 avg=3.06\n",
            "[7 | 138.43] loss=3.05 avg=3.06\n",
            "[8 | 157.92] loss=3.04 avg=3.06\n",
            "[9 | 177.19] loss=3.26 avg=3.08\n",
            "[10 | 196.44] loss=3.46 avg=3.12\n",
            "[11 | 215.65] loss=3.03 avg=3.11\n",
            "[12 | 234.70] loss=3.00 avg=3.10\n",
            "[13 | 253.84] loss=2.97 avg=3.09\n",
            "[14 | 273.02] loss=3.08 avg=3.09\n",
            "[15 | 292.36] loss=2.21 avg=3.03\n",
            "[16 | 311.26] loss=3.01 avg=3.02\n",
            "[17 | 330.19] loss=2.96 avg=3.02\n",
            "[18 | 349.14] loss=3.07 avg=3.02\n",
            "[19 | 368.29] loss=3.14 avg=3.03\n",
            "[20 | 387.18] loss=2.98 avg=3.03\n",
            "[21 | 406.16] loss=2.83 avg=3.02\n",
            "[22 | 425.06] loss=3.13 avg=3.02\n",
            "[23 | 444.14] loss=2.84 avg=3.01\n",
            "[24 | 463.41] loss=3.08 avg=3.02\n",
            "[25 | 482.54] loss=3.26 avg=3.03\n",
            "[26 | 501.59] loss=2.79 avg=3.02\n",
            "[27 | 520.74] loss=2.94 avg=3.01\n",
            "[28 | 539.77] loss=3.09 avg=3.02\n",
            "[29 | 558.85] loss=3.02 avg=3.02\n",
            "[30 | 578.01] loss=2.90 avg=3.01\n",
            "[31 | 597.24] loss=2.93 avg=3.01\n",
            "[32 | 616.27] loss=2.99 avg=3.01\n",
            "[33 | 635.35] loss=3.03 avg=3.01\n",
            "[34 | 654.39] loss=3.10 avg=3.01\n",
            "[35 | 673.53] loss=2.89 avg=3.01\n",
            "[36 | 692.76] loss=3.25 avg=3.02\n",
            "[37 | 711.86] loss=3.22 avg=3.02\n",
            "[38 | 730.95] loss=3.06 avg=3.02\n",
            "[39 | 750.08] loss=2.74 avg=3.02\n",
            "[40 | 769.59] loss=2.84 avg=3.01\n",
            "[41 | 788.93] loss=2.97 avg=3.01\n",
            "[42 | 807.91] loss=3.12 avg=3.01\n",
            "[43 | 826.98] loss=3.14 avg=3.02\n",
            "[44 | 846.33] loss=2.88 avg=3.01\n",
            "[45 | 865.25] loss=2.90 avg=3.01\n",
            "[46 | 884.30] loss=2.69 avg=3.00\n",
            "[47 | 903.41] loss=2.92 avg=3.00\n",
            "[48 | 922.52] loss=2.86 avg=3.00\n",
            "[49 | 941.61] loss=3.06 avg=3.00\n",
            "[50 | 960.70] loss=2.97 avg=3.00\n",
            "[51 | 979.74] loss=3.00 avg=3.00\n",
            "[52 | 998.75] loss=2.69 avg=2.99\n",
            "[53 | 1017.87] loss=2.83 avg=2.98\n",
            "[54 | 1036.74] loss=3.17 avg=2.99\n",
            "[55 | 1055.67] loss=2.78 avg=2.98\n",
            "[56 | 1074.64] loss=3.01 avg=2.98\n",
            "[57 | 1093.94] loss=3.09 avg=2.99\n",
            "[58 | 1112.88] loss=3.02 avg=2.99\n",
            "[59 | 1131.92] loss=2.96 avg=2.99\n",
            "[60 | 1150.92] loss=2.85 avg=2.98\n",
            "[61 | 1169.98] loss=3.48 avg=3.00\n",
            "[62 | 1188.85] loss=3.24 avg=3.00\n",
            "[63 | 1207.84] loss=2.99 avg=3.00\n",
            "[64 | 1226.77] loss=2.81 avg=3.00\n",
            "[65 | 1245.96] loss=2.65 avg=2.99\n",
            "[66 | 1264.84] loss=3.07 avg=2.99\n",
            "[67 | 1283.79] loss=2.97 avg=2.99\n",
            "[68 | 1302.68] loss=2.78 avg=2.99\n",
            "[69 | 1321.78] loss=2.93 avg=2.98\n",
            "[70 | 1340.64] loss=2.95 avg=2.98\n",
            "[71 | 1359.53] loss=2.92 avg=2.98\n",
            "[72 | 1378.61] loss=2.91 avg=2.98\n",
            "[73 | 1397.88] loss=3.16 avg=2.98\n",
            "[74 | 1416.96] loss=2.83 avg=2.98\n",
            "[75 | 1435.73] loss=2.93 avg=2.98\n",
            "[76 | 1454.58] loss=3.01 avg=2.98\n",
            "[77 | 1473.58] loss=2.77 avg=2.98\n",
            "[78 | 1492.47] loss=2.79 avg=2.97\n",
            "[79 | 1511.27] loss=3.47 avg=2.98\n",
            "[80 | 1530.05] loss=3.17 avg=2.99\n",
            "[81 | 1548.87] loss=2.48 avg=2.98\n",
            "[82 | 1568.10] loss=2.92 avg=2.98\n",
            "[83 | 1587.09] loss=3.02 avg=2.98\n",
            "[84 | 1606.11] loss=2.97 avg=2.98\n",
            "[85 | 1625.05] loss=2.74 avg=2.97\n",
            "[86 | 1644.19] loss=2.89 avg=2.97\n",
            "[87 | 1663.16] loss=1.90 avg=2.95\n",
            "[88 | 1682.07] loss=2.92 avg=2.95\n",
            "[89 | 1701.28] loss=2.74 avg=2.95\n",
            "[90 | 1720.26] loss=2.75 avg=2.95\n",
            "[91 | 1739.37] loss=2.79 avg=2.94\n",
            "[92 | 1758.26] loss=2.95 avg=2.94\n",
            "[93 | 1777.14] loss=3.14 avg=2.95\n",
            "[94 | 1796.21] loss=2.93 avg=2.95\n",
            "[95 | 1815.31] loss=2.86 avg=2.94\n",
            "[96 | 1834.16] loss=2.75 avg=2.94\n",
            "[97 | 1853.06] loss=3.06 avg=2.94\n",
            "[98 | 1871.94] loss=2.91 avg=2.94\n",
            "[99 | 1890.97] loss=2.94 avg=2.94\n",
            "[100 | 1909.88] loss=3.24 avg=2.95\n",
            "[101 | 1928.76] loss=2.72 avg=2.94\n",
            "[102 | 1947.68] loss=2.95 avg=2.94\n",
            "[103 | 1966.70] loss=3.01 avg=2.95\n",
            "[104 | 1985.65] loss=2.90 avg=2.94\n",
            "[105 | 2004.62] loss=2.98 avg=2.95\n",
            "[106 | 2023.73] loss=2.13 avg=2.93\n",
            "[107 | 2042.52] loss=3.19 avg=2.94\n",
            "[108 | 2061.55] loss=2.76 avg=2.93\n",
            "[109 | 2080.48] loss=2.64 avg=2.93\n",
            "[110 | 2099.36] loss=3.11 avg=2.93\n",
            "[111 | 2118.31] loss=2.79 avg=2.93\n",
            "[112 | 2137.39] loss=3.05 avg=2.93\n",
            "[113 | 2156.22] loss=2.77 avg=2.93\n",
            "[114 | 2175.07] loss=2.82 avg=2.93\n",
            "[115 | 2194.03] loss=2.69 avg=2.92\n",
            "[116 | 2213.04] loss=2.93 avg=2.92\n",
            "[117 | 2231.94] loss=2.82 avg=2.92\n",
            "[118 | 2250.81] loss=2.82 avg=2.92\n",
            "[119 | 2269.67] loss=2.95 avg=2.92\n",
            "[120 | 2288.59] loss=2.88 avg=2.92\n",
            "[121 | 2307.66] loss=2.77 avg=2.92\n",
            "[122 | 2327.00] loss=2.69 avg=2.92\n",
            "[123 | 2345.94] loss=2.87 avg=2.91\n",
            "[124 | 2364.90] loss=2.82 avg=2.91\n",
            "[125 | 2383.94] loss=2.87 avg=2.91\n",
            "[126 | 2402.80] loss=3.17 avg=2.92\n",
            "[127 | 2421.69] loss=2.98 avg=2.92\n",
            "[128 | 2440.50] loss=2.55 avg=2.91\n",
            "[129 | 2459.58] loss=2.89 avg=2.91\n",
            "[130 | 2478.41] loss=3.04 avg=2.91\n",
            "[131 | 2497.30] loss=3.19 avg=2.92\n",
            "[132 | 2516.13] loss=3.09 avg=2.92\n",
            "[133 | 2535.11] loss=3.11 avg=2.92\n",
            "[134 | 2553.92] loss=2.72 avg=2.92\n",
            "[135 | 2572.94] loss=2.62 avg=2.92\n",
            "[136 | 2591.86] loss=2.54 avg=2.91\n",
            "[137 | 2610.90] loss=3.00 avg=2.91\n",
            "[138 | 2629.87] loss=2.85 avg=2.91\n",
            "[139 | 2648.85] loss=3.00 avg=2.91\n",
            "[140 | 2667.78] loss=3.15 avg=2.92\n",
            "[141 | 2686.72] loss=2.89 avg=2.92\n",
            "[142 | 2705.74] loss=2.69 avg=2.91\n",
            "[143 | 2724.53] loss=2.60 avg=2.91\n",
            "[144 | 2743.47] loss=3.25 avg=2.91\n",
            "[145 | 2762.31] loss=2.84 avg=2.91\n",
            "[146 | 2781.35] loss=2.88 avg=2.91\n",
            "[147 | 2800.23] loss=2.80 avg=2.91\n",
            "[148 | 2819.18] loss=2.76 avg=2.91\n",
            "[149 | 2838.18] loss=2.81 avg=2.91\n",
            "[150 | 2857.26] loss=2.89 avg=2.91\n",
            "[151 | 2876.11] loss=3.06 avg=2.91\n",
            "[152 | 2894.90] loss=2.09 avg=2.90\n",
            "[153 | 2913.84] loss=2.97 avg=2.90\n",
            "[154 | 2932.94] loss=2.94 avg=2.90\n",
            "[155 | 2952.20] loss=2.85 avg=2.90\n",
            "[156 | 2971.15] loss=2.91 avg=2.90\n",
            "[157 | 2990.02] loss=3.03 avg=2.90\n",
            "[158 | 3008.96] loss=3.04 avg=2.90\n",
            "[159 | 3027.93] loss=2.76 avg=2.90\n",
            "[160 | 3046.73] loss=3.08 avg=2.90\n",
            "[161 | 3065.66] loss=2.52 avg=2.90\n",
            "[162 | 3084.58] loss=2.69 avg=2.90\n",
            "[163 | 3103.66] loss=2.92 avg=2.90\n",
            "[164 | 3122.52] loss=3.08 avg=2.90\n",
            "[165 | 3141.33] loss=3.01 avg=2.90\n",
            "[166 | 3160.30] loss=2.51 avg=2.89\n",
            "[167 | 3179.23] loss=2.82 avg=2.89\n",
            "[168 | 3198.17] loss=2.84 avg=2.89\n",
            "[169 | 3217.05] loss=2.79 avg=2.89\n",
            "[170 | 3235.95] loss=2.91 avg=2.89\n",
            "[171 | 3255.01] loss=2.86 avg=2.89\n",
            "[172 | 3274.00] loss=2.87 avg=2.89\n",
            "[173 | 3292.96] loss=3.14 avg=2.89\n",
            "[174 | 3311.91] loss=2.93 avg=2.89\n",
            "[175 | 3330.73] loss=2.87 avg=2.89\n",
            "[176 | 3349.72] loss=2.81 avg=2.89\n",
            "[177 | 3368.53] loss=2.95 avg=2.89\n",
            "[178 | 3387.37] loss=2.80 avg=2.89\n",
            "[179 | 3406.20] loss=2.73 avg=2.89\n",
            "[180 | 3425.10] loss=2.77 avg=2.89\n",
            "[181 | 3444.06] loss=2.78 avg=2.89\n",
            "[182 | 3463.08] loss=2.67 avg=2.89\n",
            "[183 | 3481.94] loss=2.60 avg=2.88\n",
            "[184 | 3500.85] loss=2.97 avg=2.88\n",
            "[185 | 3519.75] loss=2.77 avg=2.88\n",
            "[186 | 3538.82] loss=2.88 avg=2.88\n",
            "[187 | 3557.60] loss=3.01 avg=2.88\n",
            "[188 | 3576.91] loss=2.90 avg=2.88\n",
            "[189 | 3595.90] loss=2.56 avg=2.88\n",
            "[190 | 3614.73] loss=2.78 avg=2.88\n",
            "[191 | 3633.60] loss=2.95 avg=2.88\n",
            "[192 | 3652.55] loss=3.00 avg=2.88\n",
            "[193 | 3671.57] loss=2.79 avg=2.88\n",
            "[194 | 3690.52] loss=2.66 avg=2.88\n",
            "[195 | 3709.41] loss=3.09 avg=2.88\n",
            "[196 | 3728.53] loss=3.01 avg=2.88\n",
            "[197 | 3747.61] loss=3.26 avg=2.89\n",
            "[198 | 3766.50] loss=2.90 avg=2.89\n",
            "[199 | 3785.41] loss=2.83 avg=2.89\n",
            "[200 | 3804.34] loss=2.80 avg=2.88\n",
            "[201 | 3823.48] loss=3.01 avg=2.89\n",
            "[202 | 3842.59] loss=2.81 avg=2.88\n",
            "[203 | 3861.47] loss=2.99 avg=2.89\n",
            "[204 | 3880.57] loss=2.91 avg=2.89\n",
            "[205 | 3899.66] loss=3.12 avg=2.89\n",
            "[206 | 3918.62] loss=3.01 avg=2.89\n",
            "[207 | 3937.55] loss=3.15 avg=2.89\n",
            "[208 | 3956.45] loss=2.84 avg=2.89\n",
            "[209 | 3975.43] loss=3.01 avg=2.89\n",
            "[210 | 3994.63] loss=2.75 avg=2.89\n",
            "[211 | 4013.74] loss=2.83 avg=2.89\n",
            "[212 | 4032.64] loss=2.69 avg=2.89\n",
            "[213 | 4051.49] loss=2.48 avg=2.88\n",
            "[214 | 4070.75] loss=2.65 avg=2.88\n",
            "[215 | 4089.59] loss=2.75 avg=2.88\n",
            "[216 | 4108.52] loss=3.05 avg=2.88\n",
            "[217 | 4127.47] loss=2.77 avg=2.88\n",
            "[218 | 4146.58] loss=2.83 avg=2.88\n",
            "[219 | 4165.59] loss=2.74 avg=2.88\n",
            "[220 | 4184.58] loss=2.93 avg=2.88\n",
            "[221 | 4203.69] loss=3.13 avg=2.88\n",
            "[222 | 4222.74] loss=2.87 avg=2.88\n",
            "[223 | 4241.69] loss=3.02 avg=2.88\n",
            "[224 | 4260.61] loss=2.65 avg=2.88\n",
            "[225 | 4279.71] loss=2.77 avg=2.88\n",
            "[226 | 4298.70] loss=2.96 avg=2.88\n",
            "[227 | 4317.69] loss=2.81 avg=2.88\n",
            "[228 | 4336.66] loss=2.80 avg=2.88\n",
            "[229 | 4355.59] loss=2.58 avg=2.88\n",
            "[230 | 4374.76] loss=3.11 avg=2.88\n",
            "[231 | 4393.80] loss=3.16 avg=2.88\n",
            "[232 | 4412.80] loss=2.91 avg=2.88\n",
            "[233 | 4431.70] loss=2.95 avg=2.88\n",
            "[234 | 4450.81] loss=2.94 avg=2.88\n",
            "[235 | 4469.93] loss=2.58 avg=2.88\n",
            "[236 | 4489.00] loss=3.24 avg=2.88\n",
            "[237 | 4508.50] loss=2.79 avg=2.88\n",
            "[238 | 4527.41] loss=2.69 avg=2.88\n",
            "[239 | 4546.48] loss=3.02 avg=2.88\n",
            "[240 | 4565.56] loss=2.38 avg=2.88\n",
            "[241 | 4584.81] loss=1.30 avg=2.86\n",
            "[242 | 4603.81] loss=3.11 avg=2.86\n",
            "[243 | 4622.94] loss=2.74 avg=2.86\n",
            "[244 | 4641.93] loss=2.84 avg=2.86\n",
            "[245 | 4660.88] loss=3.21 avg=2.86\n",
            "[246 | 4679.85] loss=2.79 avg=2.86\n",
            "[247 | 4698.87] loss=2.91 avg=2.86\n",
            "[248 | 4717.90] loss=2.63 avg=2.86\n",
            "[249 | 4736.83] loss=2.92 avg=2.86\n",
            "Saving checkpoint/run1/model-250\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "al-tat-tat-tat-tat-tat-tat-tat-tat-tat-tat-tat-tat-tat-tat-tat-tat-tat-tat-tat-tat-tat_\n",
            "\n",
            "\"\n",
            "\n",
            "\"No need to explain to my girlfriend, she just is a different kind of boy, like her friend who was born and raised there but doesn't know her mum.\n",
            "\n",
            "\"\n",
            "\n",
            "\"My mum did like to eat and to be healthy and I really miss her, she always did so she did have dinner with me just in case I missed her. I'm sure it was just for dinner.\"\n",
            "\n",
            "And just then his mum says \"So what's the problem?\" and says to her friends\n",
            "\"It's always a problem, never mind anything really unusual I just make sure, if you have a problem, there's always another solution.\"\n",
            "\n",
            "So he decides to make his mum cook a big pie for his dad.\n",
            "\n",
            "Then after he is done with doing his chores he picks up a piece of bread and asks it all the same questions.\n",
            "\n",
            "And the last time he tries to start eating he just says \"no it's a little old but you want to eat something different\"\n",
            "He still doesn't like this but decides to make himself a bigger dish. All because he is not a big eater\n",
            "\"So if your dad and your mum didn't have a big thing like this you know what, I might be more interested here.\"\n",
            "\n",
            "So when he goes back home the dad asks and says\n",
            "\"Hey what's that thing, what do you mean a 'big thing'?\"\n",
            "\n",
            "So the mum, when she thinks about it for a bit, thinks of one of my old books and the first thing she likes she says to herself \"Well, it's nothing, it's just a small piece of bread. It doesn't get bigger or anything, it just sits there.\"\n",
            "\n",
            "So the mum is eating something that only she wants to eat but the question she doesn't say a single thing she really thinks about is about the next piece of bread.\n",
            "After she ends up making herself a huge dish and it just keeps getting bigger.\n",
            "\"Well why do you ask?\" she asks. \"That's like asking if you like it when your mum takes off the trousers and goes in it to show you a little bit of it, what does that do to it?\"\n",
            "\n",
            "The dad replies \"Nothing really but it's so small it's like a little teddy bear.\"\n",
            "\n",
            "The mum sits there watching the other kids eat the small plate and asks \"Why?\"\n",
            "\n",
            "\"Because it's so small it can take up five square inches of space in front of me.\"\n",
            "\n",
            "The dad says \"You know what, if it can take up that little room, then it's bigger by a foot than my little bedroom.\"\n",
            "\n",
            "The mum says \"Well, I guess so, I guess, because that's a lot more space than mine.\"\n",
            "\n",
            "The dad says \"You're kidding.. \"he turns to me just as a smile moves across his face and he says \"So why can't I ask why?\"\"\"He said you'd get an erection if I said anything that I didn't like, and he didn't. I mean...\"he shakes his head \"..he said my underwear is big.\"\n",
            "\n",
            "\"Well\" it looks as though it just has my underwear on him, then it gets the better of him as he gets bigger. \"Well, I don't actually enjoy myself, but I don't mind, and I want my dad to have an erection.\"\"Well okay he's right,\" replies the dad. \"I like that I have this long neck stretched out so I can hold it without having to put it down or try anything.\"\n",
            "\n",
            "A second time the teacher comes over to the kitchen and tells the dad that the pot is full he wants to have it so he can have it when he gets home.\n",
            "\n",
            "He looks up to see that the teacher's face is bright with excitement and excitement. Then he turns on the computer he just turned on before he realizes that a new program is running. It's called My Little Sister-Eyes by the teacher and was meant to let children know that the little sister with that long neck was still wearing a tight fitting bra, but she's actually being kept tight for all she has to do to get it to the point where she can only hold the head of her own bra down.\n",
            "So when it's time to leave, there's a new program running running so suddenly there's nothing to go on about. It doesn't seem to do any work with the way this little sister is getting the rest of the bra she has been wearing that she is not wearing to the point where she can hold the head up. So She goes back up to the kitchen and puts the\n",
            "\n",
            "[250 | 4837.37] loss=3.00 avg=2.86\n",
            "[251 | 4856.24] loss=2.78 avg=2.86\n",
            "[252 | 4875.37] loss=2.93 avg=2.86\n",
            "[253 | 4894.26] loss=2.97 avg=2.86\n",
            "[254 | 4913.19] loss=2.95 avg=2.87\n",
            "[255 | 4932.09] loss=2.80 avg=2.86\n",
            "[256 | 4951.23] loss=2.71 avg=2.86\n",
            "[257 | 4970.15] loss=2.88 avg=2.86\n",
            "[258 | 4989.06] loss=2.92 avg=2.86\n",
            "[259 | 5007.98] loss=2.89 avg=2.86\n",
            "[260 | 5026.80] loss=3.04 avg=2.87\n",
            "[261 | 5045.80] loss=2.81 avg=2.87\n",
            "[262 | 5064.76] loss=2.79 avg=2.86\n",
            "[263 | 5083.54] loss=2.82 avg=2.86\n",
            "[264 | 5102.71] loss=2.94 avg=2.87\n",
            "[265 | 5122.19] loss=2.74 avg=2.86\n",
            "[266 | 5141.09] loss=2.97 avg=2.86\n",
            "[267 | 5159.92] loss=2.68 avg=2.86\n",
            "[268 | 5178.86] loss=2.87 avg=2.86\n",
            "[269 | 5197.97] loss=2.44 avg=2.86\n",
            "[270 | 5217.00] loss=2.86 avg=2.86\n",
            "[271 | 5235.85] loss=2.80 avg=2.86\n",
            "[272 | 5254.88] loss=3.01 avg=2.86\n",
            "[273 | 5274.11] loss=3.00 avg=2.86\n",
            "[274 | 5293.24] loss=2.73 avg=2.86\n",
            "[275 | 5312.29] loss=2.73 avg=2.86\n",
            "[276 | 5331.40] loss=3.15 avg=2.86\n",
            "[277 | 5350.49] loss=2.96 avg=2.86\n",
            "[278 | 5369.54] loss=2.34 avg=2.86\n",
            "[279 | 5388.56] loss=3.12 avg=2.86\n",
            "[280 | 5407.52] loss=3.09 avg=2.86\n",
            "[281 | 5426.79] loss=2.99 avg=2.86\n",
            "[282 | 5446.11] loss=2.63 avg=2.86\n",
            "[283 | 5465.29] loss=3.08 avg=2.86\n",
            "[284 | 5484.39] loss=2.81 avg=2.86\n",
            "[285 | 5503.48] loss=2.79 avg=2.86\n",
            "[286 | 5522.66] loss=2.85 avg=2.86\n",
            "[287 | 5541.62] loss=2.66 avg=2.86\n",
            "[288 | 5560.64] loss=2.79 avg=2.86\n",
            "[289 | 5579.75] loss=2.57 avg=2.86\n",
            "[290 | 5599.03] loss=2.81 avg=2.86\n",
            "[291 | 5618.00] loss=2.81 avg=2.86\n",
            "[292 | 5637.18] loss=2.72 avg=2.85\n",
            "[293 | 5656.38] loss=3.12 avg=2.86\n",
            "[294 | 5675.68] loss=2.78 avg=2.86\n",
            "[295 | 5694.81] loss=2.63 avg=2.85\n",
            "[296 | 5714.01] loss=2.76 avg=2.85\n",
            "[297 | 5733.04] loss=2.73 avg=2.85\n",
            "[298 | 5752.35] loss=2.85 avg=2.85\n",
            "[299 | 5771.49] loss=2.82 avg=2.85\n",
            "[300 | 5790.70] loss=2.69 avg=2.85\n",
            "[301 | 5809.77] loss=3.01 avg=2.85\n",
            "[302 | 5828.94] loss=2.79 avg=2.85\n",
            "[303 | 5848.19] loss=3.26 avg=2.85\n",
            "[304 | 5867.15] loss=3.04 avg=2.86\n",
            "[305 | 5886.15] loss=2.96 avg=2.86\n",
            "[306 | 5905.30] loss=3.05 avg=2.86\n",
            "[307 | 5924.49] loss=3.03 avg=2.86\n",
            "[308 | 5943.44] loss=2.67 avg=2.86\n",
            "[309 | 5962.47] loss=2.94 avg=2.86\n",
            "[310 | 5981.44] loss=2.95 avg=2.86\n",
            "[311 | 6000.58] loss=2.68 avg=2.86\n",
            "[312 | 6019.46] loss=2.66 avg=2.86\n",
            "[313 | 6038.22] loss=2.82 avg=2.86\n",
            "[314 | 6057.37] loss=2.72 avg=2.86\n",
            "[315 | 6076.80] loss=3.01 avg=2.86\n",
            "[316 | 6096.34] loss=3.00 avg=2.86\n",
            "[317 | 6115.90] loss=2.95 avg=2.86\n",
            "[318 | 6135.26] loss=2.74 avg=2.86\n",
            "[319 | 6154.41] loss=2.72 avg=2.86\n",
            "[320 | 6173.46] loss=2.85 avg=2.86\n",
            "[321 | 6192.64] loss=2.93 avg=2.86\n",
            "[322 | 6211.83] loss=2.92 avg=2.86\n",
            "[323 | 6230.80] loss=2.92 avg=2.86\n",
            "[324 | 6250.07] loss=2.70 avg=2.86\n",
            "[325 | 6269.15] loss=2.75 avg=2.86\n",
            "[326 | 6288.42] loss=2.76 avg=2.85\n",
            "[327 | 6307.42] loss=2.87 avg=2.85\n",
            "[328 | 6326.50] loss=2.71 avg=2.85\n",
            "[329 | 6345.38] loss=2.57 avg=2.85\n",
            "[330 | 6364.53] loss=2.92 avg=2.85\n",
            "[331 | 6383.75] loss=3.03 avg=2.85\n",
            "[332 | 6402.99] loss=2.84 avg=2.85\n",
            "[333 | 6421.95] loss=2.76 avg=2.85\n",
            "[334 | 6441.02] loss=2.83 avg=2.85\n",
            "[335 | 6459.96] loss=2.99 avg=2.85\n",
            "[336 | 6479.14] loss=3.69 avg=2.86\n",
            "[337 | 6498.16] loss=2.79 avg=2.86\n",
            "[338 | 6517.08] loss=2.69 avg=2.86\n",
            "[339 | 6536.01] loss=3.15 avg=2.86\n",
            "[340 | 6555.00] loss=3.02 avg=2.86\n",
            "[341 | 6573.97] loss=3.00 avg=2.87\n",
            "[342 | 6593.11] loss=2.78 avg=2.86\n",
            "[343 | 6612.07] loss=3.01 avg=2.87\n",
            "[344 | 6631.60] loss=2.77 avg=2.86\n",
            "[345 | 6651.34] loss=2.87 avg=2.86\n",
            "[346 | 6671.24] loss=2.75 avg=2.86\n",
            "[347 | 6692.52] loss=2.88 avg=2.86\n",
            "[348 | 6713.68] loss=3.42 avg=2.87\n",
            "[349 | 6734.25] loss=3.35 avg=2.87\n",
            "[350 | 6754.57] loss=2.81 avg=2.87\n",
            "[351 | 6774.59] loss=2.94 avg=2.87\n",
            "[352 | 6795.18] loss=3.04 avg=2.88\n",
            "[353 | 6815.22] loss=2.55 avg=2.87\n",
            "[354 | 6834.99] loss=2.68 avg=2.87\n",
            "[355 | 6854.77] loss=2.77 avg=2.87\n",
            "[356 | 6874.68] loss=2.81 avg=2.87\n",
            "[357 | 6894.62] loss=2.90 avg=2.87\n",
            "[358 | 6914.64] loss=2.80 avg=2.87\n",
            "[359 | 6934.42] loss=2.84 avg=2.87\n",
            "[360 | 6954.12] loss=2.73 avg=2.87\n",
            "[361 | 6974.00] loss=2.95 avg=2.87\n",
            "[362 | 6993.83] loss=2.84 avg=2.87\n",
            "[363 | 7013.29] loss=3.06 avg=2.87\n",
            "[364 | 7032.97] loss=2.77 avg=2.87\n",
            "[365 | 7053.01] loss=3.30 avg=2.87\n",
            "[366 | 7072.53] loss=2.79 avg=2.87\n",
            "[367 | 7092.26] loss=2.67 avg=2.87\n",
            "[368 | 7111.86] loss=2.79 avg=2.87\n",
            "[369 | 7131.46] loss=2.68 avg=2.87\n",
            "[370 | 7151.02] loss=2.55 avg=2.86\n",
            "[371 | 7170.54] loss=2.55 avg=2.86\n",
            "[372 | 7190.21] loss=2.94 avg=2.86\n",
            "[373 | 7209.97] loss=2.73 avg=2.86\n",
            "[374 | 7229.54] loss=3.02 avg=2.86\n",
            "[375 | 7249.21] loss=3.04 avg=2.86\n",
            "[376 | 7268.72] loss=2.77 avg=2.86\n",
            "[377 | 7288.68] loss=3.14 avg=2.87\n",
            "[378 | 7308.78] loss=2.92 avg=2.87\n",
            "[379 | 7328.34] loss=3.12 avg=2.87\n",
            "[380 | 7348.13] loss=2.83 avg=2.87\n",
            "[381 | 7367.94] loss=2.85 avg=2.87\n",
            "[382 | 7387.62] loss=2.98 avg=2.87\n",
            "[383 | 7407.22] loss=3.79 avg=2.88\n",
            "[384 | 7426.92] loss=2.62 avg=2.88\n",
            "[385 | 7446.89] loss=2.98 avg=2.88\n",
            "[386 | 7466.87] loss=2.89 avg=2.88\n",
            "[387 | 7486.42] loss=2.61 avg=2.87\n",
            "[388 | 7506.10] loss=2.67 avg=2.87\n",
            "[389 | 7525.99] loss=2.77 avg=2.87\n",
            "[390 | 7545.43] loss=2.83 avg=2.87\n",
            "[391 | 7564.68] loss=2.85 avg=2.87\n",
            "[392 | 7584.30] loss=3.09 avg=2.87\n",
            "[393 | 7603.87] loss=2.75 avg=2.87\n",
            "[394 | 7623.53] loss=3.08 avg=2.87\n",
            "[395 | 7643.06] loss=0.88 avg=2.85\n",
            "[396 | 7662.61] loss=2.91 avg=2.85\n",
            "[397 | 7682.18] loss=2.85 avg=2.85\n",
            "[398 | 7702.08] loss=2.87 avg=2.85\n",
            "[399 | 7721.51] loss=2.72 avg=2.85\n",
            "[400 | 7740.87] loss=2.84 avg=2.85\n",
            "[401 | 7760.39] loss=2.82 avg=2.85\n",
            "[402 | 7779.96] loss=2.92 avg=2.85\n",
            "[403 | 7799.28] loss=3.18 avg=2.86\n",
            "[404 | 7818.78] loss=3.11 avg=2.86\n",
            "[405 | 7838.16] loss=2.90 avg=2.86\n",
            "[406 | 7857.71] loss=2.53 avg=2.86\n",
            "[407 | 7877.06] loss=3.30 avg=2.86\n",
            "[408 | 7896.49] loss=2.85 avg=2.86\n",
            "[409 | 7916.40] loss=2.72 avg=2.86\n",
            "[410 | 7937.74] loss=2.36 avg=2.85\n",
            "[411 | 7958.65] loss=2.84 avg=2.85\n",
            "[412 | 7979.19] loss=2.88 avg=2.85\n",
            "[413 | 7999.80] loss=3.01 avg=2.86\n",
            "[414 | 8020.71] loss=2.81 avg=2.86\n",
            "[415 | 8041.05] loss=2.87 avg=2.86\n",
            "[416 | 8061.40] loss=1.92 avg=2.85\n",
            "[417 | 8081.54] loss=2.68 avg=2.84\n",
            "[418 | 8101.80] loss=2.89 avg=2.85\n",
            "[419 | 8122.09] loss=2.66 avg=2.84\n",
            "[420 | 8142.31] loss=2.59 avg=2.84\n",
            "[421 | 8162.53] loss=2.90 avg=2.84\n",
            "[422 | 8182.88] loss=2.81 avg=2.84\n",
            "[423 | 8202.94] loss=2.87 avg=2.84\n",
            "[424 | 8223.05] loss=2.61 avg=2.84\n",
            "[425 | 8243.15] loss=2.79 avg=2.84\n",
            "[426 | 8263.26] loss=2.89 avg=2.84\n",
            "[427 | 8283.02] loss=2.88 avg=2.84\n",
            "[428 | 8302.80] loss=2.91 avg=2.84\n",
            "[429 | 8322.63] loss=2.81 avg=2.84\n",
            "[430 | 8342.60] loss=3.00 avg=2.84\n",
            "[431 | 8362.32] loss=2.58 avg=2.84\n",
            "[432 | 8382.06] loss=2.76 avg=2.84\n",
            "[433 | 8401.78] loss=2.68 avg=2.84\n",
            "[434 | 8421.80] loss=3.11 avg=2.84\n",
            "[435 | 8441.71] loss=2.86 avg=2.84\n",
            "[436 | 8461.50] loss=2.74 avg=2.84\n",
            "[437 | 8481.36] loss=2.72 avg=2.84\n",
            "[438 | 8501.27] loss=2.81 avg=2.84\n",
            "[439 | 8521.01] loss=3.04 avg=2.84\n",
            "[440 | 8540.90] loss=2.92 avg=2.84\n",
            "[441 | 8560.80] loss=2.50 avg=2.84\n",
            "[442 | 8580.70] loss=3.41 avg=2.84\n",
            "[443 | 8600.65] loss=2.82 avg=2.84\n",
            "[444 | 8620.43] loss=2.70 avg=2.84\n",
            "[445 | 8640.38] loss=2.87 avg=2.84\n",
            "[446 | 8660.30] loss=2.83 avg=2.84\n",
            "[447 | 8680.04] loss=3.00 avg=2.84\n",
            "[448 | 8699.86] loss=2.54 avg=2.84\n",
            "[449 | 8719.74] loss=3.17 avg=2.84\n",
            "[450 | 8739.71] loss=2.78 avg=2.84\n",
            "[451 | 8759.44] loss=3.14 avg=2.84\n",
            "[452 | 8779.26] loss=2.88 avg=2.85\n",
            "[453 | 8799.09] loss=2.86 avg=2.85\n",
            "[454 | 8819.08] loss=2.50 avg=2.84\n",
            "[455 | 8838.74] loss=2.94 avg=2.84\n",
            "[456 | 8858.48] loss=2.66 avg=2.84\n",
            "[457 | 8878.22] loss=2.81 avg=2.84\n",
            "[458 | 8898.33] loss=2.97 avg=2.84\n",
            "[459 | 8918.01] loss=2.74 avg=2.84\n",
            "[460 | 8938.18] loss=2.80 avg=2.84\n",
            "[461 | 8957.92] loss=3.33 avg=2.85\n",
            "[462 | 8977.77] loss=2.74 avg=2.84\n",
            "[463 | 8997.59] loss=2.87 avg=2.84\n",
            "[464 | 9017.24] loss=2.76 avg=2.84\n",
            "[465 | 9037.01] loss=2.86 avg=2.84\n",
            "[466 | 9056.96] loss=3.48 avg=2.85\n",
            "[467 | 9076.81] loss=2.75 avg=2.85\n",
            "[468 | 9096.36] loss=2.81 avg=2.85\n",
            "[469 | 9115.86] loss=2.69 avg=2.85\n",
            "[470 | 9135.72] loss=2.80 avg=2.85\n",
            "[471 | 9155.53] loss=3.00 avg=2.85\n",
            "[472 | 9175.43] loss=2.82 avg=2.85\n",
            "[473 | 9195.10] loss=2.72 avg=2.85\n",
            "[474 | 9214.78] loss=2.89 avg=2.85\n",
            "[475 | 9234.70] loss=2.93 avg=2.85\n",
            "[476 | 9254.63] loss=2.73 avg=2.85\n",
            "[477 | 9274.57] loss=2.97 avg=2.85\n",
            "[478 | 9294.45] loss=2.95 avg=2.85\n",
            "[479 | 9314.23] loss=2.68 avg=2.85\n",
            "[480 | 9334.09] loss=2.80 avg=2.85\n",
            "[481 | 9353.71] loss=2.92 avg=2.85\n",
            "[482 | 9373.58] loss=2.90 avg=2.85\n",
            "[483 | 9393.56] loss=2.70 avg=2.85\n",
            "[484 | 9413.23] loss=3.01 avg=2.85\n",
            "[485 | 9433.04] loss=2.80 avg=2.85\n",
            "[486 | 9452.85] loss=2.19 avg=2.84\n",
            "[487 | 9472.94] loss=2.62 avg=2.84\n",
            "[488 | 9492.32] loss=2.77 avg=2.84\n",
            "[489 | 9511.40] loss=2.84 avg=2.84\n",
            "[490 | 9530.37] loss=2.79 avg=2.84\n",
            "[491 | 9549.58] loss=2.60 avg=2.84\n",
            "[492 | 9568.60] loss=2.77 avg=2.83\n",
            "[493 | 9587.53] loss=2.89 avg=2.84\n",
            "[494 | 9606.49] loss=2.58 avg=2.83\n",
            "[495 | 9625.81] loss=2.16 avg=2.83\n",
            "[496 | 9645.14] loss=2.69 avg=2.82\n",
            "[497 | 9664.16] loss=2.84 avg=2.82\n",
            "[498 | 9683.35] loss=2.82 avg=2.82\n",
            "[499 | 9702.74] loss=3.18 avg=2.83\n",
            "Saving checkpoint/run1/model-500\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " looks to me now as I look to myself, I hear something that was the other night.\n",
            "\n",
            "\"Is this your girlfriend?\" says her brother-in-law, pointing up at him.\n",
            "\n",
            "\"No, she's my best friend,\" says the younger man, pointing to his girlfriend who was already staring at his shoulder.\n",
            "\n",
            "\"Well I guess so,\" I say, \"she's my best friend. You better stay at home.\"<|endoftext|>My Santa says \"I'm a little scared of the other stuff\"<|endoftext|>Dad, I'm going to be a dad!|Dad, I'm going to be a dad!\n",
            "\n",
            "Dad, I'm going to be a dad!\n",
            "\n",
            "Dad, I'm going to be a dad!\n",
            "\n",
            "Dad, I'm going to be a dad!\n",
            "\n",
            "Dad, I'm going to be a dad!\n",
            "\n",
            "Dad, I'm going to be a dad!\n",
            "\n",
            "Dad, I'm going to be a dad!\n",
            "\n",
            "Dad, I'm going to be a dad!\n",
            "\n",
            "Dad, I'm going to be a dad!\n",
            "\n",
            "Dad, I'm going to be a dad!\n",
            "\n",
            "Dad, I'm going to be a dad!\n",
            "\n",
            "Dad, I'm going to be a dad!\n",
            "\n",
            "Dad, I'm going to be a dad!\n",
            "\n",
            "Dad, I've won the contest!<|endoftext|>Why do people watch Pokemon Go?|Because they think it's fun and easy...<|endoftext|>You could get in trouble for being too tall for an exam, or for being a joke<|endoftext|>What do you get when you get too high and your dog is too tall|Oscar-in-a-pink?<|endoftext|>What's in one of those fancy hats that doesn't fit?|Mysterious <|endoftext|>What do you call a woman and her husband?|A drunk in a loo<|endoftext|>Two guys playing a game|Two guys decide to start a new game. One is a black man who is about to cross the river, the other is a Mexican. They are trying to figure out what the new player is doing. \"Happens here first\" says the white guy, which he has a good laugh at. \"H-hey I'm Mexicans\" says the black guy. \"Hey i'm sorry i'm a Mexican.\" \"No, i'm a Mexican\" says the Mexican, which he knows. \"So?\" says the Mexican. \"I'm Mexican, man. I've never been here before. I don't speak English. I don't speak much Spanish. I'm Mexican myself, too. What are you waiting for?\" \"Well, you're always Mexican on me. That's why you go to the store to get a Mexican's key.\" \"Well, yeah, then you're gonna be Mexican on me!\" said the Mexican. \"What do you have for me anyway?\" asks the black guy. \"Ya know what you're gonna do. You take the Mexican's name and take a car and go.\" \"Really?\" \"No. You're gonna see what happens when you get the car again.\" says the Mexican. \"Ya know what you're gonna do then?\" \"I'll be in this room forever.\" \"So then you're going to have to leave the country, right?\" \"Nonsense.\" <|endoftext|>You must love the idea of a Mexican in a cowboy hat|but then you'll find out the Mexicans are really into cowboy things.<|endoftext|>Why did God build a bridge|It has a nice bridge.<|endoftext|>What's the difference between a gay man and an atheist?|...it goes out on the gay side.<|endoftext|>How many years does it take to get into a job in the U.S.?|...only half. \n",
            "The job is for a person to get an ass for a long time. The job description is a bachelor's degree.<|endoftext|>Why did a doctor have to give up his house in order to have a baby?|He was too lazy and was too scared to look at the baby while it was in a crib.<|endoftext|>A man walks into a bar, walks back in and says: \"Welcome to the bar! Just what was you looking for?\"\n",
            "\n",
            "\"There, just what was you looking for?\" said an old man looking for a place to drink beer<|endoftext|>What has the first child like?|He's about three pounds more.<|endoftext|>What's the sexiest thing you've ever seen?|The dick, the egg. <|endoftext|>The best part about dating is you're\n",
            "\n",
            "[500 | 9803.47] loss=2.96 avg=2.83\n",
            "[501 | 9822.92] loss=2.88 avg=2.83\n",
            "[502 | 9842.48] loss=2.76 avg=2.83\n",
            "[503 | 9862.05] loss=2.46 avg=2.83\n",
            "[504 | 9881.49] loss=2.66 avg=2.82\n",
            "[505 | 9900.72] loss=2.98 avg=2.83\n",
            "[506 | 9919.94] loss=2.62 avg=2.82\n",
            "[507 | 9939.37] loss=2.91 avg=2.82\n",
            "[508 | 9958.66] loss=2.88 avg=2.82\n",
            "[509 | 9977.80] loss=2.61 avg=2.82\n",
            "[510 | 9996.92] loss=2.62 avg=2.82\n",
            "[511 | 10016.18] loss=3.50 avg=2.83\n",
            "[512 | 10035.51] loss=2.89 avg=2.83\n",
            "[513 | 10054.54] loss=3.06 avg=2.83\n",
            "[514 | 10073.57] loss=2.96 avg=2.83\n",
            "[515 | 10092.85] loss=2.89 avg=2.83\n",
            "[516 | 10112.26] loss=2.82 avg=2.83\n",
            "[517 | 10131.18] loss=3.15 avg=2.84\n",
            "[518 | 10150.14] loss=2.83 avg=2.84\n",
            "[519 | 10169.23] loss=2.85 avg=2.84\n",
            "[520 | 10188.44] loss=2.67 avg=2.83\n",
            "[521 | 10207.48] loss=2.76 avg=2.83\n",
            "[522 | 10226.44] loss=3.04 avg=2.83\n",
            "[523 | 10245.57] loss=2.73 avg=2.83\n",
            "[524 | 10264.74] loss=3.05 avg=2.84\n",
            "[525 | 10283.83] loss=2.93 avg=2.84\n",
            "[526 | 10302.80] loss=2.75 avg=2.84\n",
            "[527 | 10321.84] loss=2.88 avg=2.84\n",
            "[528 | 10340.91] loss=2.77 avg=2.84\n",
            "[529 | 10360.09] loss=2.79 avg=2.84\n",
            "[530 | 10379.04] loss=2.85 avg=2.84\n",
            "[531 | 10398.14] loss=2.73 avg=2.83\n",
            "[532 | 10417.32] loss=2.57 avg=2.83\n",
            "[533 | 10436.51] loss=2.95 avg=2.83\n",
            "[534 | 10455.52] loss=2.57 avg=2.83\n",
            "[535 | 10474.50] loss=2.95 avg=2.83\n",
            "[536 | 10493.49] loss=2.50 avg=2.83\n",
            "[537 | 10512.90] loss=2.85 avg=2.83\n",
            "[538 | 10531.94] loss=3.09 avg=2.83\n",
            "[539 | 10550.96] loss=2.70 avg=2.83\n",
            "[540 | 10570.06] loss=2.73 avg=2.83\n",
            "[541 | 10589.37] loss=2.47 avg=2.83\n",
            "[542 | 10608.36] loss=2.76 avg=2.82\n",
            "[543 | 10627.51] loss=2.84 avg=2.82\n",
            "[544 | 10646.53] loss=2.79 avg=2.82\n",
            "[545 | 10665.70] loss=2.94 avg=2.83\n",
            "[546 | 10684.85] loss=2.69 avg=2.82\n",
            "[547 | 10703.99] loss=2.81 avg=2.82\n",
            "[548 | 10723.17] loss=2.43 avg=2.82\n",
            "[549 | 10742.15] loss=2.84 avg=2.82\n",
            "[550 | 10761.08] loss=3.26 avg=2.82\n",
            "[551 | 10779.98] loss=2.87 avg=2.83\n",
            "[552 | 10798.85] loss=2.68 avg=2.82\n",
            "[553 | 10817.72] loss=2.99 avg=2.83\n",
            "[554 | 10836.64] loss=2.62 avg=2.82\n",
            "[555 | 10855.90] loss=3.02 avg=2.83\n",
            "[556 | 10875.71] loss=2.66 avg=2.82\n",
            "[557 | 10895.44] loss=2.71 avg=2.82\n",
            "[558 | 10915.15] loss=2.52 avg=2.82\n",
            "[559 | 10934.38] loss=2.87 avg=2.82\n",
            "[560 | 10953.82] loss=2.60 avg=2.82\n",
            "[561 | 10973.79] loss=2.88 avg=2.82\n",
            "[562 | 10994.26] loss=2.73 avg=2.82\n",
            "[563 | 11013.59] loss=2.69 avg=2.82\n",
            "[564 | 11033.17] loss=2.91 avg=2.82\n",
            "[565 | 11052.28] loss=2.64 avg=2.82\n",
            "[566 | 11071.65] loss=2.93 avg=2.82\n",
            "[567 | 11090.95] loss=2.52 avg=2.81\n",
            "[568 | 11110.09] loss=1.59 avg=2.80\n",
            "[569 | 11129.37] loss=3.28 avg=2.81\n",
            "[570 | 11148.71] loss=2.72 avg=2.81\n",
            "[571 | 11168.15] loss=2.83 avg=2.81\n",
            "[572 | 11187.40] loss=2.94 avg=2.81\n",
            "[573 | 11206.50] loss=2.87 avg=2.81\n",
            "[574 | 11225.81] loss=2.77 avg=2.81\n",
            "[575 | 11245.10] loss=2.88 avg=2.81\n",
            "[576 | 11264.23] loss=2.65 avg=2.81\n",
            "[577 | 11283.29] loss=2.27 avg=2.80\n",
            "[578 | 11302.29] loss=2.50 avg=2.80\n",
            "[579 | 11321.51] loss=2.96 avg=2.80\n",
            "[580 | 11340.88] loss=2.65 avg=2.80\n",
            "[581 | 11359.76] loss=2.90 avg=2.80\n",
            "[582 | 11378.67] loss=2.87 avg=2.80\n",
            "[583 | 11397.80] loss=3.10 avg=2.80\n",
            "[584 | 11416.61] loss=2.54 avg=2.80\n",
            "[585 | 11435.57] loss=2.91 avg=2.80\n",
            "[586 | 11454.52] loss=2.82 avg=2.80\n",
            "[587 | 11473.77] loss=2.87 avg=2.80\n",
            "[588 | 11492.75] loss=2.76 avg=2.80\n",
            "[589 | 11511.75] loss=2.87 avg=2.80\n",
            "[590 | 11530.61] loss=2.80 avg=2.80\n",
            "[591 | 11549.67] loss=3.20 avg=2.81\n",
            "[592 | 11568.72] loss=2.68 avg=2.81\n",
            "[593 | 11587.66] loss=2.91 avg=2.81\n",
            "[594 | 11606.65] loss=2.66 avg=2.80\n",
            "[595 | 11625.61] loss=2.42 avg=2.80\n",
            "[596 | 11644.89] loss=2.96 avg=2.80\n",
            "[597 | 11664.01] loss=2.83 avg=2.80\n",
            "[598 | 11683.03] loss=3.02 avg=2.80\n",
            "[599 | 11702.13] loss=2.99 avg=2.81\n",
            "[600 | 11721.35] loss=2.35 avg=2.80\n",
            "[601 | 11740.32] loss=2.93 avg=2.80\n",
            "[602 | 11759.33] loss=3.07 avg=2.81\n",
            "[603 | 11778.28] loss=2.80 avg=2.81\n",
            "[604 | 11797.30] loss=2.86 avg=2.81\n",
            "[605 | 11816.16] loss=3.00 avg=2.81\n",
            "[606 | 11835.02] loss=2.70 avg=2.81\n",
            "[607 | 11853.97] loss=2.56 avg=2.81\n",
            "[608 | 11873.03] loss=2.51 avg=2.80\n",
            "[609 | 11892.09] loss=2.90 avg=2.80\n",
            "[610 | 11911.17] loss=2.97 avg=2.80\n",
            "[611 | 11930.10] loss=2.69 avg=2.80\n",
            "[612 | 11949.05] loss=2.94 avg=2.81\n",
            "[613 | 11968.37] loss=2.61 avg=2.80\n",
            "[614 | 11987.33] loss=2.70 avg=2.80\n",
            "[615 | 12006.14] loss=2.56 avg=2.80\n",
            "[616 | 12024.95] loss=2.88 avg=2.80\n",
            "[617 | 12043.99] loss=2.80 avg=2.80\n",
            "[618 | 12062.83] loss=2.72 avg=2.80\n",
            "[619 | 12081.85] loss=2.96 avg=2.80\n",
            "[620 | 12100.64] loss=2.98 avg=2.80\n",
            "[621 | 12119.75] loss=2.80 avg=2.80\n",
            "[622 | 12138.68] loss=2.47 avg=2.80\n",
            "[623 | 12157.46] loss=2.86 avg=2.80\n",
            "[624 | 12176.49] loss=2.63 avg=2.80\n",
            "[625 | 12195.50] loss=0.02 avg=2.77\n",
            "[626 | 12214.44] loss=2.34 avg=2.77\n",
            "[627 | 12233.42] loss=2.92 avg=2.77\n",
            "[628 | 12252.35] loss=2.69 avg=2.77\n",
            "[629 | 12271.52] loss=2.80 avg=2.77\n",
            "[630 | 12290.68] loss=3.01 avg=2.77\n",
            "[631 | 12309.58] loss=3.39 avg=2.78\n",
            "[632 | 12328.75] loss=2.96 avg=2.78\n",
            "[633 | 12347.64] loss=2.80 avg=2.78\n",
            "[634 | 12366.80] loss=2.54 avg=2.78\n",
            "[635 | 12385.81] loss=2.68 avg=2.77\n",
            "[636 | 12404.73] loss=2.87 avg=2.78\n",
            "[637 | 12423.63] loss=2.56 avg=2.77\n",
            "[638 | 12442.70] loss=2.61 avg=2.77\n",
            "[639 | 12461.68] loss=2.67 avg=2.77\n",
            "[640 | 12480.51] loss=2.94 avg=2.77\n",
            "[641 | 12499.66] loss=2.56 avg=2.77\n",
            "[642 | 12518.72] loss=2.76 avg=2.77\n",
            "[643 | 12537.79] loss=3.04 avg=2.77\n",
            "[644 | 12556.73] loss=3.11 avg=2.78\n",
            "[645 | 12575.89] loss=2.85 avg=2.78\n",
            "[646 | 12595.12] loss=2.74 avg=2.78\n",
            "[647 | 12614.28] loss=2.56 avg=2.77\n",
            "[648 | 12633.37] loss=2.96 avg=2.78\n",
            "[649 | 12652.41] loss=2.31 avg=2.77\n",
            "[650 | 12671.39] loss=2.38 avg=2.77\n",
            "[651 | 12690.77] loss=2.77 avg=2.77\n",
            "[652 | 12709.94] loss=2.66 avg=2.77\n",
            "[653 | 12728.94] loss=2.80 avg=2.77\n",
            "[654 | 12747.90] loss=2.68 avg=2.77\n",
            "[655 | 12766.90] loss=2.80 avg=2.77\n",
            "[656 | 12785.74] loss=2.97 avg=2.77\n",
            "[657 | 12804.66] loss=2.92 avg=2.77\n",
            "[658 | 12823.45] loss=2.70 avg=2.77\n",
            "[659 | 12842.43] loss=2.83 avg=2.77\n",
            "[660 | 12861.32] loss=2.91 avg=2.77\n",
            "[661 | 12880.16] loss=2.83 avg=2.77\n",
            "[662 | 12899.20] loss=2.86 avg=2.77\n",
            "[663 | 12918.17] loss=2.88 avg=2.77\n",
            "[664 | 12937.30] loss=2.51 avg=2.77\n",
            "[665 | 12956.24] loss=2.82 avg=2.77\n",
            "[666 | 12975.17] loss=2.75 avg=2.77\n",
            "[667 | 12994.38] loss=3.00 avg=2.77\n",
            "[668 | 13013.49] loss=2.67 avg=2.77\n",
            "[669 | 13032.37] loss=2.89 avg=2.77\n",
            "[670 | 13051.35] loss=3.08 avg=2.78\n",
            "[671 | 13070.34] loss=2.65 avg=2.78\n",
            "[672 | 13089.47] loss=2.37 avg=2.77\n",
            "[673 | 13108.45] loss=2.76 avg=2.77\n",
            "[674 | 13127.46] loss=2.54 avg=2.77\n",
            "[675 | 13146.44] loss=2.77 avg=2.77\n",
            "[676 | 13165.76] loss=2.95 avg=2.77\n",
            "[677 | 13184.77] loss=2.70 avg=2.77\n",
            "[678 | 13203.87] loss=2.74 avg=2.77\n",
            "[679 | 13222.70] loss=2.76 avg=2.77\n",
            "[680 | 13241.56] loss=3.22 avg=2.77\n",
            "[681 | 13260.47] loss=2.94 avg=2.78\n",
            "[682 | 13279.35] loss=3.00 avg=2.78\n",
            "[683 | 13298.33] loss=2.79 avg=2.78\n",
            "[684 | 13317.18] loss=2.71 avg=2.78\n",
            "[685 | 13336.27] loss=2.87 avg=2.78\n",
            "[686 | 13355.23] loss=2.75 avg=2.78\n",
            "[687 | 13374.11] loss=2.39 avg=2.77\n",
            "[688 | 13393.12] loss=2.98 avg=2.78\n",
            "[689 | 13412.14] loss=2.56 avg=2.77\n",
            "[690 | 13431.26] loss=2.96 avg=2.78\n",
            "[691 | 13450.19] loss=2.49 avg=2.77\n",
            "[692 | 13469.21] loss=2.89 avg=2.77\n",
            "[693 | 13488.30] loss=2.71 avg=2.77\n",
            "[694 | 13507.44] loss=2.91 avg=2.78\n",
            "[695 | 13526.64] loss=3.03 avg=2.78\n",
            "[696 | 13545.73] loss=2.43 avg=2.77\n",
            "[697 | 13564.77] loss=2.85 avg=2.78\n",
            "[698 | 13583.79] loss=2.70 avg=2.77\n",
            "[699 | 13602.86] loss=2.75 avg=2.77\n",
            "[700 | 13621.81] loss=2.83 avg=2.77\n",
            "[701 | 13640.75] loss=3.18 avg=2.78\n",
            "[702 | 13659.97] loss=2.69 avg=2.78\n",
            "[703 | 13679.02] loss=2.69 avg=2.78\n",
            "[704 | 13697.97] loss=2.65 avg=2.78\n",
            "[705 | 13717.01] loss=2.74 avg=2.78\n",
            "[706 | 13735.99] loss=3.20 avg=2.78\n",
            "[707 | 13754.83] loss=2.74 avg=2.78\n",
            "[708 | 13773.59] loss=2.80 avg=2.78\n",
            "[709 | 13792.40] loss=2.78 avg=2.78\n",
            "[710 | 13811.31] loss=2.58 avg=2.78\n",
            "[711 | 13830.33] loss=2.85 avg=2.78\n",
            "[712 | 13849.32] loss=2.67 avg=2.78\n",
            "[713 | 13868.25] loss=2.34 avg=2.77\n",
            "[714 | 13887.26] loss=3.09 avg=2.78\n",
            "[715 | 13906.40] loss=2.98 avg=2.78\n",
            "[716 | 13925.40] loss=3.00 avg=2.78\n",
            "[717 | 13944.29] loss=2.66 avg=2.78\n",
            "[718 | 13963.32] loss=3.04 avg=2.78\n",
            "[719 | 13982.40] loss=2.79 avg=2.78\n",
            "[720 | 14001.36] loss=3.21 avg=2.79\n",
            "[721 | 14020.26] loss=2.59 avg=2.78\n",
            "[722 | 14039.26] loss=2.86 avg=2.78\n",
            "[723 | 14058.33] loss=2.84 avg=2.79\n",
            "[724 | 14077.27] loss=2.50 avg=2.78\n",
            "[725 | 14096.30] loss=2.14 avg=2.78\n",
            "[726 | 14115.36] loss=2.90 avg=2.78\n",
            "[727 | 14134.40] loss=2.77 avg=2.78\n",
            "[728 | 14153.23] loss=2.64 avg=2.78\n",
            "[729 | 14172.03] loss=2.72 avg=2.78\n",
            "[730 | 14190.76] loss=2.62 avg=2.77\n",
            "[731 | 14209.70] loss=2.78 avg=2.77\n",
            "[732 | 14228.48] loss=2.73 avg=2.77\n",
            "[733 | 14247.24] loss=2.62 avg=2.77\n",
            "[734 | 14265.81] loss=2.54 avg=2.77\n",
            "[735 | 14284.55] loss=2.73 avg=2.77\n",
            "[736 | 14303.45] loss=2.75 avg=2.77\n",
            "[737 | 14322.05] loss=1.01 avg=2.75\n",
            "[738 | 14340.92] loss=2.86 avg=2.75\n",
            "[739 | 14359.81] loss=2.79 avg=2.75\n",
            "[740 | 14378.64] loss=2.89 avg=2.75\n",
            "[741 | 14397.34] loss=2.99 avg=2.76\n",
            "[742 | 14416.24] loss=3.15 avg=2.76\n",
            "[743 | 14435.30] loss=3.12 avg=2.76\n",
            "[744 | 14454.59] loss=2.93 avg=2.77\n",
            "[745 | 14473.61] loss=2.68 avg=2.76\n",
            "[746 | 14492.65] loss=2.77 avg=2.76\n",
            "[747 | 14511.64] loss=2.87 avg=2.77\n",
            "[748 | 14530.78] loss=2.91 avg=2.77\n",
            "[749 | 14549.95] loss=2.84 avg=2.77\n",
            "Saving checkpoint/run1/model-750\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " he said.\n",
            "\n",
            "<|endoftext|>This is going to be a great show...|Everyone walks up, hands out as if on a leash.<|endoftext|>I was the head of a circus...|... but I'm just a child. <|endoftext|>What do you call a girl with a tattoo of Hitler?|The Joker. <|endoftext|>I was born as a vegetarian but now I am a vegan|It was my idea of a long drive home that I drive to work every day to try to figure out how to drive. \n",
            "\n",
            "My dad was a vegetable and the only way he could convince his parents that he liked his food was with a meal out. \n",
            "\n",
            "After a while as he was getting more and more satisfied he started asking the kids about himself. \n",
            "\n",
            "They both got a little bit curious. \n",
            "\n",
            "One of them was so obsessed with his vegetarian diet he decided to put his friends at risk. \n",
            "\n",
            "One of them, an old college student, had the nerve to buy a copy of the book. \n",
            "His sister couldn't believe she just had to stop eating all his fruit at once. \n",
            "\n",
            "She then got a bit disappointed when she tried to save her family money by getting her son to buy it. \n",
            "\n",
            "So, to her surprise, Harry went down and bought it just for him. \n",
            "\n",
            "My dad had the same reaction too and didn't believe that it was a good idea. \n",
            "\n",
            "The first week, his brother, when he was just twelve, was driving across America and suddenly it wasn't going well. \n",
            "\n",
            "Two kids were in the back bumper of his car, they wanted to jump out of their seats on bumper to bumper. \n",
            ". . . .\n",
            "\n",
            "My father jumped out and called my brother in to tell him everyone was going to be okay. \n",
            "\n",
            "My mother jumped and looked at her son as he was driving.\n",
            "\n",
            "My mom saw her kid and looked over her shoulder.\n",
            "\n",
            "My dad then called me father on the phone asking for a ride back to bed. \n",
            "\n",
            "My mom said she thought my dad was crazy but she got tired of calling him like that and took the call anyway. \n",
            "\n",
            "My dad looked at the girl and said \"Well, what happened, I just took the phone call from our room?\" \n",
            "\n",
            "My dad said, \"You can tell me if you think it's weird.\" \n",
            "\n",
            "My mom looked at me and asked, \"Dad, are you having trouble calling my mom?\" \n",
            "\n",
            "My dad said, \"Oh, mommy, I haven't asked her that question.\"\n",
            "\n",
            "My eyes widened. \n",
            "\n",
            "My eyes widened again and said, \"Dad, if it's been that long since you last saw her, are you still with her?\" \n",
            "\n",
            "My dad said, \"No, no sir, just the way she is.\" As he tried to answer, she started screaming and screaming into the living room and into the bathroom and she broke off and was bleeding from her mouth and nose. \n",
            "\n",
            "My dad looked down at the girl and said, \"Mommy, I hope it isn't too bad. I don't think we've seen anyone break so easily like that at home. No Mommy, your sister is bleeding from her mouth and nose. My dad doesn't have a brother who's in here to keep her going. How could she have been crying like that when her brothers were here? How old was she before the divorce? She's still here and nobody's ever asked her that question. Where did she go?\" My mother replied. \"We told you that her mom was just talking to her mother who had just entered into her husband's relationship with her.\"\n",
            "<|endoftext|>Jokes are great.|I'm a big joke, but then I'm really just a joke. <|endoftext|>A woman was walking home from work...|When she stumbled across some odd man in the bushes, she yelled: \"That's the guy! He's the one who told me she was pregnant.\"   \n",
            "When she saw the man she was shocked. Her head was pounding and she started sobbing.<|endoftext|>A woman came in to my apartment from work today to get some milk for dinner. She asked me what I was doing so I said \"I just came in to go get the milk. I just walked in\"<|endoftext|>Two men meet in the streets, one of them says \"What in heavens happened?|...and asks \"Who's the other man?\" The other man says : \"That was a bit of a surprise...I've known many men in the past who would have been happy with a stranger.\"<|endoftext|>How many women do you have in a house?|Two.<|\n",
            "\n",
            "[750 | 14650.48] loss=2.60 avg=2.77\n",
            "[751 | 14669.38] loss=2.86 avg=2.77\n",
            "[752 | 14688.37] loss=2.69 avg=2.77\n",
            "[753 | 14707.49] loss=2.83 avg=2.77\n",
            "[754 | 14726.18] loss=2.94 avg=2.77\n",
            "[755 | 14744.92] loss=2.56 avg=2.77\n",
            "[756 | 14763.99] loss=2.83 avg=2.77\n",
            "[757 | 14783.13] loss=2.88 avg=2.77\n",
            "[758 | 14802.16] loss=2.69 avg=2.77\n",
            "[759 | 14821.15] loss=2.78 avg=2.77\n",
            "[760 | 14840.23] loss=2.61 avg=2.77\n",
            "[761 | 14859.27] loss=2.82 avg=2.77\n",
            "[762 | 14878.36] loss=2.65 avg=2.77\n",
            "[763 | 14897.03] loss=2.83 avg=2.77\n",
            "[764 | 14915.98] loss=2.96 avg=2.77\n",
            "[765 | 14934.94] loss=2.40 avg=2.76\n",
            "[766 | 14953.98] loss=2.34 avg=2.76\n",
            "[767 | 14973.07] loss=2.83 avg=2.76\n",
            "[768 | 14992.13] loss=2.58 avg=2.76\n",
            "[769 | 15011.27] loss=2.58 avg=2.76\n",
            "[770 | 15030.50] loss=2.57 avg=2.76\n",
            "[771 | 15050.29] loss=2.70 avg=2.76\n",
            "[772 | 15070.20] loss=2.87 avg=2.76\n",
            "[773 | 15089.80] loss=3.09 avg=2.76\n",
            "[774 | 15109.25] loss=2.68 avg=2.76\n",
            "[775 | 15128.56] loss=2.62 avg=2.76\n",
            "[776 | 15147.86] loss=2.71 avg=2.76\n",
            "[777 | 15167.18] loss=2.95 avg=2.76\n",
            "[778 | 15186.57] loss=2.77 avg=2.76\n",
            "[779 | 15205.75] loss=2.63 avg=2.76\n",
            "[780 | 15224.92] loss=3.02 avg=2.76\n",
            "[781 | 15244.07] loss=2.76 avg=2.76\n",
            "[782 | 15263.41] loss=2.88 avg=2.76\n",
            "[783 | 15282.90] loss=2.81 avg=2.76\n",
            "[784 | 15303.18] loss=2.46 avg=2.76\n",
            "[785 | 15323.32] loss=2.94 avg=2.76\n",
            "[786 | 15343.40] loss=2.85 avg=2.76\n",
            "[787 | 15362.92] loss=2.83 avg=2.76\n",
            "[788 | 15383.01] loss=2.72 avg=2.76\n",
            "[789 | 15402.50] loss=2.64 avg=2.76\n",
            "[790 | 15422.22] loss=3.20 avg=2.77\n",
            "[791 | 15441.92] loss=2.37 avg=2.76\n",
            "[792 | 15461.39] loss=2.80 avg=2.76\n",
            "[793 | 15481.00] loss=2.69 avg=2.76\n",
            "[794 | 15500.47] loss=2.81 avg=2.76\n",
            "[795 | 15519.93] loss=2.70 avg=2.76\n",
            "[796 | 15539.37] loss=2.60 avg=2.76\n",
            "[797 | 15558.52] loss=2.79 avg=2.76\n",
            "[798 | 15577.72] loss=2.63 avg=2.76\n",
            "[799 | 15597.06] loss=2.97 avg=2.76\n",
            "[800 | 15617.08] loss=3.39 avg=2.77\n",
            "[801 | 15637.32] loss=2.60 avg=2.76\n",
            "[802 | 15656.54] loss=2.71 avg=2.76\n",
            "[803 | 15676.11] loss=2.89 avg=2.77\n",
            "[804 | 15695.63] loss=2.69 avg=2.76\n",
            "[805 | 15714.83] loss=2.81 avg=2.77\n",
            "[806 | 15734.15] loss=2.66 avg=2.76\n",
            "[807 | 15753.72] loss=2.82 avg=2.76\n",
            "[808 | 15773.05] loss=2.69 avg=2.76\n",
            "[809 | 15792.23] loss=2.64 avg=2.76\n",
            "[810 | 15811.53] loss=3.25 avg=2.77\n",
            "[811 | 15831.09] loss=2.82 avg=2.77\n",
            "[812 | 15850.30] loss=2.63 avg=2.77\n",
            "[813 | 15869.53] loss=3.06 avg=2.77\n",
            "[814 | 15888.76] loss=2.79 avg=2.77\n",
            "[815 | 15907.98] loss=2.72 avg=2.77\n",
            "[816 | 15927.01] loss=2.62 avg=2.77\n",
            "[817 | 15946.16] loss=2.62 avg=2.77\n",
            "[818 | 15965.14] loss=2.49 avg=2.76\n",
            "[819 | 15984.38] loss=2.68 avg=2.76\n",
            "[820 | 16003.89] loss=2.67 avg=2.76\n",
            "[821 | 16022.94] loss=2.92 avg=2.76\n",
            "[822 | 16042.00] loss=2.67 avg=2.76\n",
            "[823 | 16061.22] loss=2.79 avg=2.76\n",
            "[824 | 16080.39] loss=2.64 avg=2.76\n",
            "[825 | 16099.74] loss=2.11 avg=2.75\n",
            "[826 | 16118.92] loss=2.51 avg=2.75\n",
            "[827 | 16138.04] loss=2.83 avg=2.75\n",
            "[828 | 16157.24] loss=2.95 avg=2.76\n",
            "[829 | 16176.22] loss=3.05 avg=2.76\n",
            "[830 | 16195.27] loss=2.71 avg=2.76\n",
            "[831 | 16214.19] loss=2.75 avg=2.76\n",
            "[832 | 16233.37] loss=2.98 avg=2.76\n",
            "[833 | 16252.42] loss=2.77 avg=2.76\n",
            "[834 | 16271.31] loss=2.92 avg=2.76\n",
            "[835 | 16290.33] loss=2.70 avg=2.76\n",
            "[836 | 16309.65] loss=2.78 avg=2.76\n",
            "[837 | 16328.71] loss=2.61 avg=2.76\n",
            "[838 | 16347.64] loss=2.69 avg=2.76\n",
            "[839 | 16366.82] loss=2.95 avg=2.76\n",
            "[840 | 16385.78] loss=2.77 avg=2.76\n",
            "[841 | 16404.97] loss=2.72 avg=2.76\n",
            "[842 | 16424.05] loss=3.02 avg=2.76\n",
            "[843 | 16443.12] loss=2.70 avg=2.76\n",
            "[844 | 16462.20] loss=2.74 avg=2.76\n",
            "[845 | 16481.31] loss=2.97 avg=2.76\n",
            "[846 | 16500.27] loss=2.97 avg=2.77\n",
            "[847 | 16519.12] loss=2.63 avg=2.76\n",
            "[848 | 16538.02] loss=2.56 avg=2.76\n",
            "[849 | 16557.25] loss=2.67 avg=2.76\n",
            "[850 | 16576.30] loss=2.81 avg=2.76\n",
            "[851 | 16595.39] loss=2.65 avg=2.76\n",
            "[852 | 16614.94] loss=2.50 avg=2.76\n",
            "[853 | 16635.24] loss=1.82 avg=2.75\n",
            "[854 | 16655.27] loss=2.79 avg=2.75\n",
            "[855 | 16674.96] loss=2.45 avg=2.75\n",
            "[856 | 16694.41] loss=2.40 avg=2.74\n",
            "[857 | 16713.98] loss=2.85 avg=2.74\n",
            "[858 | 16733.26] loss=2.89 avg=2.75\n",
            "[859 | 16752.43] loss=2.98 avg=2.75\n",
            "[860 | 16771.74] loss=3.38 avg=2.75\n",
            "[861 | 16791.06] loss=2.72 avg=2.75\n",
            "[862 | 16810.36] loss=2.60 avg=2.75\n",
            "[863 | 16829.49] loss=2.49 avg=2.75\n",
            "[864 | 16848.67] loss=2.81 avg=2.75\n",
            "[865 | 16867.70] loss=2.69 avg=2.75\n",
            "[866 | 16886.94] loss=2.97 avg=2.75\n",
            "[867 | 16905.90] loss=2.78 avg=2.75\n",
            "[868 | 16925.19] loss=2.86 avg=2.75\n",
            "[869 | 16944.45] loss=2.86 avg=2.75\n",
            "[870 | 16963.91] loss=2.85 avg=2.76\n",
            "[871 | 16982.96] loss=2.95 avg=2.76\n",
            "[872 | 17001.93] loss=2.46 avg=2.75\n",
            "[873 | 17020.94] loss=2.86 avg=2.76\n",
            "[874 | 17040.18] loss=2.69 avg=2.76\n",
            "[875 | 17059.15] loss=2.61 avg=2.75\n",
            "[876 | 17078.12] loss=2.71 avg=2.75\n",
            "[877 | 17097.26] loss=2.59 avg=2.75\n",
            "[878 | 17116.52] loss=2.66 avg=2.75\n",
            "[879 | 17135.76] loss=2.63 avg=2.75\n",
            "[880 | 17155.22] loss=2.83 avg=2.75\n",
            "[881 | 17174.69] loss=2.98 avg=2.75\n",
            "[882 | 17194.12] loss=3.39 avg=2.76\n",
            "[883 | 17213.65] loss=2.71 avg=2.76\n",
            "[884 | 17233.00] loss=2.81 avg=2.76\n",
            "[885 | 17252.40] loss=2.95 avg=2.76\n",
            "[886 | 17271.76] loss=2.95 avg=2.76\n",
            "[887 | 17291.11] loss=2.66 avg=2.76\n",
            "[888 | 17310.35] loss=2.58 avg=2.76\n",
            "[889 | 17329.60] loss=2.85 avg=2.76\n",
            "[890 | 17348.68] loss=2.71 avg=2.76\n",
            "[891 | 17367.88] loss=3.24 avg=2.77\n",
            "[892 | 17386.79] loss=2.89 avg=2.77\n",
            "[893 | 17405.97] loss=2.70 avg=2.77\n",
            "[894 | 17425.32] loss=2.70 avg=2.76\n",
            "[895 | 17444.42] loss=2.50 avg=2.76\n",
            "[896 | 17463.48] loss=2.70 avg=2.76\n",
            "[897 | 17482.47] loss=2.50 avg=2.76\n",
            "[898 | 17501.55] loss=3.06 avg=2.76\n",
            "[899 | 17520.67] loss=2.76 avg=2.76\n",
            "[900 | 17539.61] loss=2.69 avg=2.76\n",
            "[901 | 17559.09] loss=2.80 avg=2.76\n",
            "[902 | 17578.22] loss=2.85 avg=2.76\n",
            "[903 | 17597.13] loss=3.04 avg=2.77\n",
            "[904 | 17616.30] loss=2.81 avg=2.77\n",
            "[905 | 17635.40] loss=2.80 avg=2.77\n",
            "[906 | 17654.57] loss=2.80 avg=2.77\n",
            "[907 | 17673.75] loss=2.55 avg=2.76\n",
            "[908 | 17693.01] loss=2.84 avg=2.77\n",
            "[909 | 17712.04] loss=3.43 avg=2.77\n",
            "[910 | 17731.08] loss=2.81 avg=2.77\n",
            "[911 | 17750.12] loss=2.75 avg=2.77\n",
            "[912 | 17769.24] loss=2.56 avg=2.77\n",
            "[913 | 17788.28] loss=2.65 avg=2.77\n",
            "[914 | 17807.34] loss=3.36 avg=2.77\n",
            "[915 | 17826.31] loss=2.87 avg=2.78\n",
            "[916 | 17845.54] loss=2.72 avg=2.77\n",
            "[917 | 17864.69] loss=2.82 avg=2.78\n",
            "[918 | 17883.94] loss=2.67 avg=2.77\n",
            "[919 | 17902.90] loss=2.75 avg=2.77\n",
            "[920 | 17922.05] loss=2.98 avg=2.78\n",
            "[921 | 17941.14] loss=2.48 avg=2.77\n",
            "[922 | 17960.03] loss=2.94 avg=2.77\n",
            "[923 | 17978.92] loss=2.74 avg=2.77\n",
            "[924 | 17998.10] loss=2.45 avg=2.77\n",
            "[925 | 18017.09] loss=3.23 avg=2.78\n",
            "[926 | 18036.15] loss=2.72 avg=2.78\n",
            "[927 | 18055.32] loss=2.87 avg=2.78\n",
            "[928 | 18074.37] loss=2.60 avg=2.77\n",
            "[929 | 18093.62] loss=2.88 avg=2.78\n",
            "[930 | 18112.61] loss=2.91 avg=2.78\n",
            "[931 | 18131.62] loss=2.60 avg=2.78\n",
            "[932 | 18150.74] loss=2.62 avg=2.77\n",
            "[933 | 18170.14] loss=2.86 avg=2.77\n",
            "[934 | 18189.58] loss=2.37 avg=2.77\n",
            "[935 | 18208.81] loss=2.57 avg=2.77\n",
            "[936 | 18227.80] loss=2.99 avg=2.77\n",
            "[937 | 18246.83] loss=3.11 avg=2.77\n",
            "[938 | 18265.90] loss=2.79 avg=2.77\n",
            "[939 | 18284.74] loss=3.46 avg=2.78\n",
            "[940 | 18303.57] loss=2.83 avg=2.78\n",
            "[941 | 18322.71] loss=3.00 avg=2.78\n",
            "[942 | 18341.73] loss=2.72 avg=2.78\n",
            "[943 | 18360.86] loss=2.94 avg=2.78\n",
            "[944 | 18379.98] loss=2.96 avg=2.79\n",
            "[945 | 18399.20] loss=3.46 avg=2.79\n",
            "[946 | 18418.33] loss=3.05 avg=2.80\n",
            "[947 | 18437.44] loss=2.65 avg=2.79\n",
            "[948 | 18456.69] loss=2.82 avg=2.79\n",
            "[949 | 18476.10] loss=2.67 avg=2.79\n",
            "[950 | 18495.51] loss=2.73 avg=2.79\n",
            "[951 | 18514.61] loss=2.72 avg=2.79\n",
            "[952 | 18533.58] loss=2.80 avg=2.79\n",
            "[953 | 18552.73] loss=2.92 avg=2.79\n",
            "[954 | 18572.03] loss=2.78 avg=2.79\n",
            "[955 | 18591.23] loss=2.75 avg=2.79\n",
            "[956 | 18610.50] loss=2.66 avg=2.79\n",
            "[957 | 18629.51] loss=2.69 avg=2.79\n",
            "[958 | 18648.79] loss=3.25 avg=2.80\n",
            "[959 | 18667.80] loss=2.53 avg=2.79\n",
            "[960 | 18686.85] loss=2.82 avg=2.79\n",
            "[961 | 18705.91] loss=2.80 avg=2.79\n",
            "[962 | 18724.99] loss=2.93 avg=2.79\n",
            "[963 | 18744.06] loss=2.77 avg=2.79\n",
            "[964 | 18763.05] loss=2.69 avg=2.79\n",
            "[965 | 18782.05] loss=2.76 avg=2.79\n",
            "[966 | 18801.37] loss=2.65 avg=2.79\n",
            "[967 | 18820.50] loss=2.30 avg=2.79\n",
            "[968 | 18839.59] loss=2.67 avg=2.79\n",
            "[969 | 18858.68] loss=2.89 avg=2.79\n",
            "[970 | 18877.63] loss=2.70 avg=2.79\n",
            "[971 | 18896.82] loss=3.16 avg=2.79\n",
            "[972 | 18915.81] loss=2.81 avg=2.79\n",
            "[973 | 18934.67] loss=2.79 avg=2.79\n",
            "[974 | 18953.75] loss=2.79 avg=2.79\n",
            "[975 | 18973.07] loss=2.84 avg=2.79\n",
            "[976 | 18991.96] loss=2.54 avg=2.79\n",
            "[977 | 19010.92] loss=2.73 avg=2.79\n",
            "[978 | 19030.01] loss=2.94 avg=2.79\n",
            "[979 | 19049.07] loss=2.90 avg=2.79\n",
            "[980 | 19068.07] loss=3.16 avg=2.79\n",
            "[981 | 19087.27] loss=2.59 avg=2.79\n",
            "[982 | 19106.50] loss=2.94 avg=2.79\n",
            "[983 | 19125.48] loss=3.07 avg=2.80\n",
            "[984 | 19144.40] loss=2.69 avg=2.79\n",
            "[985 | 19163.28] loss=2.37 avg=2.79\n",
            "[986 | 19182.12] loss=2.75 avg=2.79\n",
            "[987 | 19200.98] loss=2.75 avg=2.79\n",
            "[988 | 19220.12] loss=2.55 avg=2.79\n",
            "[989 | 19239.05] loss=2.57 avg=2.78\n",
            "[990 | 19258.16] loss=2.94 avg=2.79\n",
            "[991 | 19277.31] loss=2.68 avg=2.79\n",
            "[992 | 19296.62] loss=2.65 avg=2.78\n",
            "[993 | 19315.87] loss=2.87 avg=2.78\n",
            "[994 | 19335.06] loss=2.88 avg=2.79\n",
            "[995 | 19354.45] loss=2.82 avg=2.79\n",
            "[996 | 19374.10] loss=2.78 avg=2.79\n",
            "[997 | 19393.50] loss=3.02 avg=2.79\n",
            "[998 | 19413.41] loss=2.72 avg=2.79\n",
            "[999 | 19433.24] loss=2.72 avg=2.79\n",
            "Saving checkpoint/run1/model-1000\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " laid on by a friend, who would eventually tell the story.\n",
            "\n",
            "A few days later the two met up and began talking about their new position on the moon.\n",
            "\n",
            "\"You see my friend?\" said the woman.\n",
            "\n",
            "\"Yes, thanks to those great astronauts,\" replied the man, \"I'm a scientist, a chemist, and a mathematician.\"\n",
            "\n",
            "\"Is that ok?\" asked the woman.\n",
            "\n",
            "\"Nope. Just a couple of notes. This morning, I went to sleep with a meteor to my left and a scientist to my right.\"\n",
            "\n",
            "The man had the woman think for a moment and looked at her. As she had seen them before, she was thinking about something else.\n",
            "\n",
            "\"What's my wife's favorite time to spend with a scientist?\" asked the woman. \"What does one day teach her?\"\n",
            "\n",
            "\"That one day is one I think is a bit long,\" answered the man. \"That is, she's on a trip to the moon two years ahead, just before we get back to work and they are talking about how our space will be like so we should be able to find more suitable spaces.\"\n",
            "\n",
            "\"Okay; a little short, but there aren't any spare spaces on the moon right now. The scientists have been thinking about that for a while. We can't do anything with the resources we had, for instance. We need to find a way to take away any waste.\"\n",
            "\n",
            "The woman got upset and yelled, \"Wait a minute. I've been thinking about this for so long, and I'm starting to catch fire. What do you mean? I'm starting to get angry and a little worried.\"\n",
            "\n",
            "The woman, a little annoyed, turned to the scientist and said, \"I just saw one like that one last night, by the way. What did the meteor find?\"\n",
            "\n",
            "\"I said that its found on a far left corner of the moon. The moon was a little too hot the morning before. So now we'll have to find a better approach.\"\n",
            "\n",
            "The scientist looked at her impatiently and said, \"It is, but the moon's a little too hot.\"\n",
            "\n",
            "The woman couldn't help but get angry. \"Wait one second. You were thinking about that? This guy just happened to show up at my office with one of your favorite scientists. Now you think you can just go about it?\"\n",
            "\n",
            "\"Oh, if I didn't know more about our moon, I'd call that an emergency, too,\" replied the woman.\n",
            "\n",
            "\"Ok,\" added the man, \"then what about what about the meteor?\"\n",
            "\n",
            "The woman said, \"well, I've never been the brightest in the earth, but I'm a doctor!\"\n",
            "\n",
            "\"You mean you're talking about a scientist?\" the scientist asked.\n",
            "\n",
            "And the woman said, \"I don't know, what did you mean?\"\n",
            "\n",
            "The man said, \"Wait a second. I heard about that meteor today, by the way.\"\n",
            "\n",
            "When the scientist reached up to take a look, he said, \"I am worried about something. My wife and I recently decided to take a holiday.\"\n",
            "\n",
            "\"OK, that's not a big deal,\" said the scientist. \"We just moved into our new quarters in the middle of a crowded bar. We had always been friends, and I was really nervous. But it felt natural. We went out drinking. As I walked in, I remembered that this was a very cold weekend, and that I had to go to work in the morning. So I decided to go out with my boyfriend. We went for a walk, but soon I realized that the rain wasn't falling very well. So I said, 'Wait a second, what happened to that meteor?' \"\n",
            "\n",
            "The scientist, surprised, replied with, \"Oh, I heard the meteor was coming to town. The first one went off over the edge, right in front of our house.\"\n",
            "\n",
            "Then the second one disappeared. It dropped in, came back, and continued with its course. I can only hope that the meteor was coming from a different direction. There will be a lot of rain down there, and I need more shelter so I can help put it to bed.\n",
            "\n",
            "As luck would have it, the meteor came straight up the street. What happened next was not in danger of falling, but of falling out of the sky. My guess is that there was some sort of tornado behind the clouds. But the meteor, while it was moving, actually went on to fall into a corner, and was caught and flattened. A lot of people are shocked and have been in a panic about what happened to the meteor.\n",
            "\n",
            "I can only hope it was not a tornado. I would hope I'm right. But I don't know. Maybe a meteor fell out of the sky, and the clouds weren't moving.\n",
            "\n",
            "Maybe something was up. Maybe something was happening on the\n",
            "\n",
            "[1000 | 19537.99] loss=2.67 avg=2.79\n",
            "[1001 | 19558.49] loss=2.71 avg=2.79\n",
            "[1002 | 19579.14] loss=2.62 avg=2.78\n",
            "[1003 | 19599.69] loss=2.99 avg=2.79\n",
            "[1004 | 19620.67] loss=2.94 avg=2.79\n",
            "[1005 | 19641.62] loss=2.55 avg=2.78\n",
            "[1006 | 19662.53] loss=2.88 avg=2.79\n",
            "[1007 | 19683.50] loss=2.85 avg=2.79\n",
            "[1008 | 19704.85] loss=2.58 avg=2.78\n",
            "[1009 | 19726.24] loss=2.71 avg=2.78\n",
            "[1010 | 19747.44] loss=2.84 avg=2.78\n",
            "[1011 | 19768.88] loss=3.15 avg=2.79\n",
            "[1012 | 19789.95] loss=2.66 avg=2.79\n",
            "[1013 | 19811.23] loss=2.51 avg=2.78\n",
            "[1014 | 19832.67] loss=2.90 avg=2.78\n",
            "[1015 | 19854.15] loss=2.78 avg=2.78\n",
            "[1016 | 19875.46] loss=2.81 avg=2.78\n",
            "[1017 | 19896.49] loss=2.77 avg=2.78\n",
            "[1018 | 19917.51] loss=2.87 avg=2.79\n",
            "[1019 | 19938.85] loss=3.23 avg=2.79\n",
            "[1020 | 19959.86] loss=2.67 avg=2.79\n",
            "[1021 | 19980.99] loss=2.31 avg=2.78\n",
            "[1022 | 20002.30] loss=2.33 avg=2.78\n",
            "[1023 | 20023.19] loss=2.70 avg=2.78\n",
            "[1024 | 20044.41] loss=2.76 avg=2.78\n",
            "[1025 | 20065.22] loss=2.63 avg=2.78\n",
            "[1026 | 20086.26] loss=2.82 avg=2.78\n",
            "[1027 | 20107.22] loss=2.51 avg=2.77\n",
            "[1028 | 20128.16] loss=2.69 avg=2.77\n",
            "[1029 | 20149.55] loss=2.81 avg=2.77\n",
            "[1030 | 20171.10] loss=2.61 avg=2.77\n",
            "[1031 | 20192.71] loss=2.61 avg=2.77\n",
            "[1032 | 20214.00] loss=2.64 avg=2.77\n",
            "[1033 | 20235.42] loss=3.10 avg=2.77\n",
            "[1034 | 20257.10] loss=2.73 avg=2.77\n",
            "[1035 | 20278.55] loss=2.84 avg=2.77\n",
            "[1036 | 20299.90] loss=2.48 avg=2.77\n",
            "[1037 | 20321.53] loss=2.73 avg=2.77\n",
            "[1038 | 20342.85] loss=2.62 avg=2.77\n",
            "[1039 | 20364.38] loss=2.84 avg=2.77\n",
            "[1040 | 20385.27] loss=2.62 avg=2.77\n",
            "[1041 | 20406.25] loss=2.60 avg=2.77\n",
            "[1042 | 20427.66] loss=2.71 avg=2.77\n",
            "[1043 | 20448.82] loss=2.96 avg=2.77\n",
            "[1044 | 20469.87] loss=3.05 avg=2.77\n",
            "[1045 | 20490.60] loss=2.74 avg=2.77\n",
            "[1046 | 20511.55] loss=2.71 avg=2.77\n",
            "[1047 | 20532.85] loss=2.60 avg=2.77\n",
            "[1048 | 20554.40] loss=2.63 avg=2.77\n",
            "[1049 | 20575.76] loss=3.20 avg=2.77\n",
            "[1050 | 20597.20] loss=3.26 avg=2.78\n",
            "[1051 | 20618.54] loss=2.14 avg=2.77\n",
            "[1052 | 20639.97] loss=2.58 avg=2.77\n",
            "[1053 | 20661.98] loss=3.16 avg=2.77\n",
            "[1054 | 20683.63] loss=2.85 avg=2.77\n",
            "[1055 | 20705.06] loss=2.72 avg=2.77\n",
            "[1056 | 20726.88] loss=2.57 avg=2.77\n",
            "[1057 | 20748.56] loss=2.63 avg=2.77\n",
            "[1058 | 20770.40] loss=2.60 avg=2.77\n",
            "[1059 | 20792.28] loss=2.58 avg=2.76\n",
            "[1060 | 20814.21] loss=2.49 avg=2.76\n",
            "[1061 | 20836.40] loss=2.49 avg=2.76\n",
            "[1062 | 20858.02] loss=2.73 avg=2.76\n",
            "[1063 | 20879.78] loss=2.30 avg=2.75\n",
            "[1064 | 20901.70] loss=2.85 avg=2.76\n",
            "[1065 | 20923.63] loss=2.99 avg=2.76\n",
            "[1066 | 20945.85] loss=2.59 avg=2.76\n",
            "[1067 | 20968.11] loss=2.78 avg=2.76\n",
            "[1068 | 20990.20] loss=3.01 avg=2.76\n",
            "[1069 | 21012.27] loss=2.65 avg=2.76\n",
            "[1070 | 21034.21] loss=2.84 avg=2.76\n",
            "[1071 | 21056.21] loss=2.98 avg=2.76\n",
            "[1072 | 21078.24] loss=2.83 avg=2.76\n",
            "[1073 | 21099.98] loss=2.98 avg=2.76\n",
            "[1074 | 21121.55] loss=2.74 avg=2.76\n",
            "[1075 | 21143.18] loss=2.69 avg=2.76\n",
            "[1076 | 21164.87] loss=2.89 avg=2.76\n",
            "[1077 | 21186.36] loss=2.86 avg=2.76\n",
            "[1078 | 21208.15] loss=2.67 avg=2.76\n",
            "[1079 | 21230.08] loss=2.76 avg=2.76\n",
            "[1080 | 21252.25] loss=2.82 avg=2.76\n",
            "[1081 | 21274.40] loss=2.65 avg=2.76\n",
            "[1082 | 21296.54] loss=2.76 avg=2.76\n",
            "[1083 | 21318.79] loss=2.97 avg=2.77\n",
            "[1084 | 21340.75] loss=2.87 avg=2.77\n",
            "[1085 | 21362.99] loss=2.77 avg=2.77\n",
            "[1086 | 21385.37] loss=2.73 avg=2.77\n",
            "[1087 | 21407.61] loss=2.69 avg=2.77\n",
            "[1088 | 21429.86] loss=2.88 avg=2.77\n",
            "[1089 | 21452.29] loss=2.85 avg=2.77\n",
            "[1090 | 21474.33] loss=3.01 avg=2.77\n",
            "[1091 | 21496.16] loss=2.61 avg=2.77\n",
            "[1092 | 21518.16] loss=2.73 avg=2.77\n",
            "[1093 | 21540.19] loss=2.84 avg=2.77\n",
            "[1094 | 21562.40] loss=2.75 avg=2.77\n",
            "[1095 | 21584.43] loss=2.72 avg=2.77\n",
            "[1096 | 21606.47] loss=2.61 avg=2.77\n",
            "[1097 | 21628.39] loss=2.85 avg=2.77\n",
            "[1098 | 21650.24] loss=2.50 avg=2.76\n",
            "[1099 | 21672.25] loss=2.74 avg=2.76\n",
            "[1100 | 21694.13] loss=2.81 avg=2.76\n",
            "[1101 | 21716.28] loss=2.91 avg=2.77\n",
            "[1102 | 21738.05] loss=2.75 avg=2.77\n",
            "[1103 | 21760.01] loss=1.50 avg=2.75\n",
            "[1104 | 21781.84] loss=2.86 avg=2.75\n",
            "[1105 | 21804.07] loss=2.83 avg=2.75\n",
            "[1106 | 21826.15] loss=2.78 avg=2.76\n",
            "[1107 | 21848.14] loss=2.60 avg=2.75\n",
            "[1108 | 21870.07] loss=2.60 avg=2.75\n",
            "[1109 | 21891.98] loss=2.82 avg=2.75\n",
            "[1110 | 21914.25] loss=2.97 avg=2.75\n",
            "[1111 | 21936.13] loss=2.79 avg=2.76\n",
            "[1112 | 21958.34] loss=2.48 avg=2.75\n",
            "[1113 | 21980.30] loss=2.51 avg=2.75\n",
            "[1114 | 22002.23] loss=2.48 avg=2.75\n",
            "[1115 | 22024.26] loss=2.73 avg=2.75\n",
            "[1116 | 22046.65] loss=2.87 avg=2.75\n",
            "[1117 | 22068.51] loss=2.95 avg=2.75\n",
            "[1118 | 22090.25] loss=2.83 avg=2.75\n",
            "[1119 | 22111.70] loss=2.69 avg=2.75\n",
            "[1120 | 22133.23] loss=2.79 avg=2.75\n",
            "[1121 | 22154.86] loss=2.87 avg=2.75\n",
            "[1122 | 22176.39] loss=2.64 avg=2.75\n",
            "[1123 | 22198.10] loss=2.65 avg=2.75\n",
            "[1124 | 22220.04] loss=2.66 avg=2.75\n",
            "[1125 | 22241.87] loss=3.12 avg=2.75\n",
            "[1126 | 22263.81] loss=2.51 avg=2.75\n",
            "[1127 | 22285.73] loss=3.03 avg=2.75\n",
            "[1128 | 22307.32] loss=2.89 avg=2.75\n",
            "[1129 | 22328.48] loss=2.68 avg=2.75\n",
            "[1130 | 22349.98] loss=3.23 avg=2.76\n",
            "[1131 | 22371.35] loss=2.92 avg=2.76\n",
            "[1132 | 22392.47] loss=3.01 avg=2.76\n",
            "[1133 | 22413.81] loss=2.71 avg=2.76\n",
            "[1134 | 22435.56] loss=2.70 avg=2.76\n",
            "[1135 | 22456.99] loss=2.93 avg=2.76\n",
            "[1136 | 22478.67] loss=2.82 avg=2.76\n",
            "[1137 | 22500.74] loss=2.85 avg=2.76\n",
            "[1138 | 22522.59] loss=2.49 avg=2.76\n",
            "[1139 | 22544.28] loss=2.59 avg=2.76\n",
            "[1140 | 22566.37] loss=2.94 avg=2.76\n",
            "[1141 | 22588.26] loss=2.67 avg=2.76\n",
            "[1142 | 22610.27] loss=2.88 avg=2.76\n",
            "[1143 | 22632.00] loss=2.66 avg=2.76\n",
            "[1144 | 22653.78] loss=2.63 avg=2.76\n",
            "[1145 | 22675.82] loss=2.58 avg=2.76\n",
            "[1146 | 22697.85] loss=2.66 avg=2.76\n",
            "[1147 | 22719.99] loss=2.74 avg=2.76\n",
            "[1148 | 22741.94] loss=2.75 avg=2.76\n",
            "[1149 | 22763.97] loss=2.75 avg=2.76\n",
            "[1150 | 22785.98] loss=2.66 avg=2.76\n",
            "[1151 | 22808.04] loss=2.60 avg=2.75\n",
            "[1152 | 22830.20] loss=2.84 avg=2.76\n",
            "[1153 | 22851.96] loss=2.67 avg=2.75\n",
            "[1154 | 22873.81] loss=2.62 avg=2.75\n",
            "[1155 | 22895.83] loss=2.69 avg=2.75\n",
            "[1156 | 22917.98] loss=2.86 avg=2.75\n",
            "[1157 | 22940.04] loss=2.60 avg=2.75\n",
            "[1158 | 22962.35] loss=2.92 avg=2.75\n",
            "[1159 | 22984.58] loss=2.82 avg=2.75\n",
            "[1160 | 23006.80] loss=2.94 avg=2.76\n",
            "[1161 | 23028.74] loss=3.03 avg=2.76\n",
            "[1162 | 23050.76] loss=2.50 avg=2.76\n",
            "[1163 | 23072.72] loss=2.92 avg=2.76\n",
            "[1164 | 23094.76] loss=2.81 avg=2.76\n",
            "[1165 | 23116.82] loss=2.93 avg=2.76\n",
            "[1166 | 23139.11] loss=2.92 avg=2.76\n",
            "[1167 | 23161.36] loss=3.39 avg=2.77\n",
            "[1168 | 23183.59] loss=2.58 avg=2.77\n",
            "[1169 | 23205.68] loss=2.47 avg=2.76\n",
            "[1170 | 23227.71] loss=2.31 avg=2.76\n",
            "[1171 | 23250.03] loss=2.97 avg=2.76\n",
            "[1172 | 23272.11] loss=2.40 avg=2.76\n",
            "[1173 | 23294.35] loss=2.92 avg=2.76\n",
            "[1174 | 23316.80] loss=2.33 avg=2.75\n",
            "[1175 | 23339.14] loss=2.72 avg=2.75\n",
            "[1176 | 23361.19] loss=2.71 avg=2.75\n",
            "[1177 | 23383.36] loss=2.56 avg=2.75\n",
            "[1178 | 23405.76] loss=2.81 avg=2.75\n",
            "[1179 | 23428.28] loss=2.88 avg=2.75\n",
            "[1180 | 23450.62] loss=3.07 avg=2.76\n",
            "[1181 | 23472.86] loss=2.75 avg=2.76\n",
            "[1182 | 23495.30] loss=2.54 avg=2.75\n",
            "[1183 | 23517.56] loss=2.68 avg=2.75\n",
            "[1184 | 23539.88] loss=2.66 avg=2.75\n",
            "[1185 | 23562.41] loss=2.77 avg=2.75\n",
            "[1186 | 23584.61] loss=1.82 avg=2.74\n",
            "[1187 | 23606.83] loss=2.90 avg=2.75\n",
            "[1188 | 23628.93] loss=2.88 avg=2.75\n",
            "[1189 | 23651.30] loss=2.77 avg=2.75\n",
            "[1190 | 23673.50] loss=2.69 avg=2.75\n",
            "[1191 | 23695.79] loss=2.45 avg=2.74\n",
            "[1192 | 23717.97] loss=2.70 avg=2.74\n",
            "[1193 | 23740.29] loss=2.65 avg=2.74\n",
            "[1194 | 23762.41] loss=2.55 avg=2.74\n",
            "[1195 | 23784.60] loss=2.49 avg=2.74\n",
            "[1196 | 23806.79] loss=2.50 avg=2.74\n",
            "[1197 | 23828.93] loss=2.52 avg=2.73\n",
            "[1198 | 23850.93] loss=2.78 avg=2.73\n",
            "[1199 | 23873.05] loss=2.82 avg=2.73\n",
            "[1200 | 23895.26] loss=2.51 avg=2.73\n",
            "[1201 | 23917.17] loss=2.86 avg=2.73\n",
            "[1202 | 23939.21] loss=2.93 avg=2.74\n",
            "[1203 | 23961.18] loss=1.94 avg=2.73\n",
            "[1204 | 23983.31] loss=2.90 avg=2.73\n",
            "[1205 | 24005.08] loss=2.53 avg=2.73\n",
            "[1206 | 24027.02] loss=3.00 avg=2.73\n",
            "[1207 | 24049.29] loss=2.80 avg=2.73\n",
            "[1208 | 24071.39] loss=2.87 avg=2.73\n",
            "[1209 | 24093.47] loss=2.83 avg=2.73\n",
            "[1210 | 24115.59] loss=2.89 avg=2.73\n",
            "[1211 | 24137.98] loss=2.56 avg=2.73\n",
            "[1212 | 24160.33] loss=2.34 avg=2.73\n",
            "[1213 | 24182.64] loss=2.38 avg=2.73\n",
            "[1214 | 24205.26] loss=2.73 avg=2.73\n",
            "[1215 | 24227.57] loss=2.54 avg=2.72\n",
            "[1216 | 24249.85] loss=2.75 avg=2.72\n",
            "[1217 | 24272.05] loss=2.64 avg=2.72\n",
            "[1218 | 24294.63] loss=3.07 avg=2.73\n",
            "[1219 | 24316.83] loss=2.54 avg=2.72\n",
            "[1220 | 24339.01] loss=2.67 avg=2.72\n",
            "[1221 | 24361.41] loss=2.91 avg=2.73\n",
            "[1222 | 24383.98] loss=2.69 avg=2.73\n",
            "[1223 | 24406.46] loss=2.81 avg=2.73\n",
            "[1224 | 24428.74] loss=2.64 avg=2.73\n",
            "[1225 | 24451.09] loss=2.75 avg=2.73\n",
            "[1226 | 24473.53] loss=2.97 avg=2.73\n",
            "[1227 | 24495.92] loss=2.55 avg=2.73\n",
            "[1228 | 24518.29] loss=2.90 avg=2.73\n",
            "[1229 | 24540.84] loss=2.70 avg=2.73\n",
            "[1230 | 24563.25] loss=2.82 avg=2.73\n",
            "[1231 | 24585.73] loss=2.72 avg=2.73\n",
            "[1232 | 24608.09] loss=2.71 avg=2.73\n",
            "[1233 | 24630.35] loss=3.13 avg=2.73\n",
            "[1234 | 24652.71] loss=2.91 avg=2.73\n",
            "[1235 | 24674.97] loss=2.56 avg=2.73\n",
            "[1236 | 24697.26] loss=2.75 avg=2.73\n",
            "[1237 | 24719.63] loss=2.86 avg=2.73\n",
            "[1238 | 24742.07] loss=2.69 avg=2.73\n",
            "[1239 | 24764.36] loss=2.77 avg=2.73\n",
            "[1240 | 24786.52] loss=2.65 avg=2.73\n",
            "[1241 | 24808.62] loss=3.53 avg=2.74\n",
            "[1242 | 24830.92] loss=2.65 avg=2.74\n",
            "[1243 | 24853.48] loss=0.07 avg=2.71\n",
            "[1244 | 24875.91] loss=2.77 avg=2.71\n",
            "[1245 | 24898.25] loss=3.03 avg=2.72\n",
            "[1246 | 24920.77] loss=2.79 avg=2.72\n",
            "[1247 | 24943.21] loss=2.69 avg=2.72\n",
            "[1248 | 24965.35] loss=2.84 avg=2.72\n",
            "[1249 | 24987.59] loss=2.74 avg=2.72\n",
            "Saving checkpoint/run1/model-1250\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "<You're always telling me, baby!|But how exactly were your eyes supposed to match??<|endoftext|>How did the woman with this face lose her virginity?|She told the guy, \"I have a girl with a penis\".<|endoftext|>There were a lot of pirates in the jungle...|..<|endoftext|>A man says: \"So I was walking in a park and suddenly I saw a giant fish coming out of a nearby tree.\"|This was before the internet was banned. <|endoftext|>A woman walks into a bar and the bartender says: \"I'd like to have a glass of water. So I've been drinking my own blood! I have no way of knowing what you did in that particular place, though. Tell me, then. Who you think you are is going to win, by any chance?''\n",
            "''You know, I don't think I have a choice. I can't drink blood with you. I'm too young, I've got nothing to lose, and it'll be over within a second. You may try it.<|endoftext|>HOLY FUCK!!!|It was really rough last night.<|endoftext|>It's important to keep in touch when you're in jail|<|endoftext|>What time do you like to be a hobo?||A few hours before my first meal.<|endoftext|>Why did the old man say 'I will never know the secret of the black hole'?|Because he had no idea it was white noise!\n",
            "\n",
            "And what should I read when I get a little hungry?\n",
            "<|endoftext|>Why aren't there any black-hole porn sites anymore?|So they got a lot of white-hole porn. <|endoftext|>When I went to church yesterday, I saw a couple of old women on the edge of the church, with a lot of hair on their shoulders.|They looked like they were in some sort of war zone.<|endoftext|>What's different about the internet?|I can still find them, and I still get them. <|endoftext|>A man walks into a bar and is told :|\n",
            "You've been drinking to the point of drinking...|No, that's not a bar. It's a bar. <|endoftext|>What is the farthest thing you could go a night?|Nothing! <|endoftext|>What is the best method for beating the drug problem?|Migraine<|endoftext|>I've always been a good listener|It's really good when people tell you they want something they don't have, to help you. \n",
            "\n",
            "I like to say I don't hear what people say when they hear it. \n",
            "\n",
            "But I can hear how it feels.<|endoftext|>A guy walks into a bar and says to a bar patron:\n",
            "\n",
            "Hey man, where is there a gay man with a gun?\n",
            "\n",
            "Hey man, where is there a gay gentleman with a gun?\n",
            "\n",
            "Hey man it's only gay and you should hang it off if you can't get it to hang out\n",
            "\n",
            "Hey man it's only gay and I can't get it to hang out <|endoftext|>What do you call a person who can't sleep?|A Dead Kid<|endoftext|>Why does a girl not love black guys because they all wear white shit?|Because they're only real good for a shit load.<|endoftext|>So somebody has hacked a computer with a virus...|...it's really getting pretty expensive.<|endoftext|>A man walks into a bar and the bartender says: Why is it only black and only black and only black and only black?|\"I don't know about that, but I'm not really black!\"<|endoftext|>One night a girl is having trouble sleeping.|So this girl, with a bad headache, sits down and asks the man which way to move her feet.\n",
            "\n",
            "The man gives her a thumbs up gesture and says \"Get to the other side of the room so that the foot won't touch the ground.\"<|endoftext|>My girlfriend asked me to kill herself|And if she died, my parents would have taken me.<|endoftext|>What do you call a child of six?|A four. A fourteen<|endoftext|>Did you hear about the guy who died in school?|He was an idiot.<|endoftext|>Why did the old man go to the gym?|He had it on his belt.<|endoftext|>I want my wife to make fun of me...|I want her to joke about my boobs, and her\n",
            "\n",
            "[1250 | 25097.40] loss=2.34 avg=2.72\n",
            "[1251 | 25119.54] loss=2.98 avg=2.72\n",
            "[1252 | 25141.88] loss=2.94 avg=2.72\n",
            "[1253 | 25164.37] loss=3.08 avg=2.72\n",
            "[1254 | 25187.03] loss=2.87 avg=2.73\n",
            "[1255 | 25209.47] loss=2.78 avg=2.73\n",
            "[1256 | 25231.98] loss=2.62 avg=2.72\n",
            "[1257 | 25254.57] loss=2.54 avg=2.72\n",
            "[1258 | 25277.30] loss=2.68 avg=2.72\n",
            "[1259 | 25299.82] loss=3.13 avg=2.73\n",
            "[1260 | 25322.56] loss=2.71 avg=2.73\n",
            "[1261 | 25344.76] loss=2.58 avg=2.73\n",
            "[1262 | 25367.29] loss=2.67 avg=2.72\n",
            "[1263 | 25389.75] loss=0.04 avg=2.70\n",
            "[1264 | 25412.02] loss=2.72 avg=2.70\n",
            "[1265 | 25434.23] loss=2.42 avg=2.70\n",
            "[1266 | 25457.40] loss=2.93 avg=2.70\n",
            "[1267 | 25480.57] loss=2.47 avg=2.70\n",
            "[1268 | 25503.61] loss=2.52 avg=2.69\n",
            "[1269 | 25526.51] loss=3.20 avg=2.70\n",
            "[1270 | 25549.25] loss=2.32 avg=2.69\n",
            "[1271 | 25572.27] loss=3.75 avg=2.71\n",
            "[1272 | 25595.39] loss=2.74 avg=2.71\n",
            "[1273 | 25618.17] loss=2.71 avg=2.71\n",
            "[1274 | 25641.21] loss=2.56 avg=2.70\n",
            "[1275 | 25664.82] loss=2.54 avg=2.70\n",
            "[1276 | 25687.70] loss=2.91 avg=2.70\n",
            "[1277 | 25710.50] loss=2.59 avg=2.70\n",
            "interrupted\n",
            "Saving checkpoint/run1/model-1278\n",
            "WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FkBMqF2xhB0"
      },
      "source": [
        "#Saving the model\n",
        "!cp -r /content/gpt-2/checkpoint/run1/* /content/gpt-2/models/117M/"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66DOzhTSyGGQ",
        "outputId": "ad002b9a-9b69-4cdf-d057-f0abf3e298e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#You can have fun now generating random jokes from an AI :)\n",
        "!python3 src/generate_unconditional_samples.py --top_k 40 --temperature 0.7 --length 200 --nsamples 10"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/root/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/root/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From src/generate_unconditional_samples.py:52: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2020-11-08 22:29:57.669833: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2020-11-08 22:29:57.673498: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
            "2020-11-08 22:29:57.673667: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2b13800 executing computations on platform Host. Devices:\n",
            "2020-11-08 22:29:57.673694: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "WARNING:tensorflow:From src/generate_unconditional_samples.py:54: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:16: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From src/generate_unconditional_samples.py:63: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "2020-11-08 22:30:00.578510: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
            "2020-11-08 22:30:00.587987: W tensorflow/core/framework/allocator.cc:107] Allocation of 154389504 exceeds 10% of system memory.\n",
            "======================================== SAMPLE 1 ========================================\n",
            "The same thing happened to me when I was in the hospital.\n",
            "\n",
            "I had a bad headache the day the doctor came.\n",
            "\n",
            "I was so sick that I had to take the medicine.\n",
            "\n",
            "I was on the ward when he came in.\n",
            "\n",
            "I was not sure what happened then.\n",
            "\n",
            "He said he was taking some of the medicine.\n",
            "\n",
            "He gave me the medicine.\n",
            "\n",
            "I said, \"I am not going to take it.\"\n",
            "\n",
            "He said, \"No, no.\"\n",
            "\n",
            "I said, \"What?\"\n",
            "\n",
            "He said, \"You don't have any of it.\"\n",
            "\n",
            "I said, \"No, no, you have the stuff.\"\n",
            "\n",
            "He said, \"You don't have the stuff, I mean, you don't have the stuff.\"\n",
            "\n",
            "I said, \"What?\"\n",
            "\n",
            "He said, \"You don't have the stuff, I mean, you don't have the stuff, I mean,\n",
            "======================================== SAMPLE 2 ========================================\n",
            "\"What's the difference between a child and a cow?\"\n",
            "\n",
            "\"A cow is a cow and a cow is a cow.\"<|endoftext|>I'm a little late to the party, but I will be back soon|<|endoftext|>What's the difference between a redneck and a white supremacist?|Redneck hates white people, but he's also a racist.<|endoftext|>The black man is a hero on the street.|He's been shot in the head. He's got a bullet in his head<|endoftext|>What do you call a guy who has a little bit of alcohol?|A drunk. \n",
            "\n",
            "I'll bet you the guy who was driving him around the block was a drunk.<|endoftext|>I see my wife is dying.|She's got a dead baby and the doctor is not sure what to do with it. \n",
            "\n",
            "So I went\n",
            "======================================== SAMPLE 3 ========================================\n",
            "In response to a question about his career, he said:\n",
            "\n",
            "\"I've been in the game for over 10 years and I've never had a bad time. I'm a really good player; I've been in the game for nearly 10 years and I've never had a bad time.\"\n",
            "\n",
            "And he was right.\n",
            "\n",
            "When it comes to the game, he's certainly not the first to say that.\n",
            "\n",
            "He was speaking about the success of his career.\n",
            "\n",
            "He's been playing for over 10 years. He's never had a bad time.\n",
            "\n",
            "He's a really good player.\n",
            "\n",
            "He's been in the game for nearly 10 years and he's never had a bad time.\n",
            "\n",
            "He's a really good player.\n",
            "\n",
            "He's been in the game for nearly 10 years and he's never had a bad time.\n",
            "\n",
            "He's a really good player.\n",
            "\n",
            "He's been in the game for over 10 years and he\n",
            "======================================== SAMPLE 4 ========================================\n",
            ">What's the difference between a man and a woman?\n",
            "\n",
            "-I don't know...\n",
            "\n",
            "-You just mean that one time you went to the toilet and the other time you tried to put a condom on it...\n",
            "\n",
            "-Not a problem...\n",
            "\n",
            "-It's my fault, I don't know...\n",
            "\n",
            "-You're not a man...\n",
            "\n",
            "-Not at all.\n",
            "\n",
            "-You're just a woman?\n",
            "\n",
            "-No, I'm a man. But I still have a penis...<|endoftext|>What did the redneck say to the black guy?|He's a racist!<|endoftext|>Trying to tell a joke..|What's the difference between a man and a woman?\n",
            "\n",
            "-The woman is getting laid, and the man is getting laid.<|endoftext|>What do you call a woman who is too fat to carry a baby?|A woman who\n",
            "======================================== SAMPLE 5 ========================================\n",
            "\"You're the best, I'm the best!\"\n",
            "\n",
            "\"No, I'm the best.\"\n",
            "\n",
            "\"Don't get it, girl. You're the best.\"\n",
            "\n",
            "\"No, I'm the best!\"\n",
            "\n",
            "\"I'm the best, girl. You're the best.\"\n",
            "\n",
            "\"I'm the best, girl. You're the best.\"\n",
            "\n",
            "\"You're the best, I'm the best!\"\n",
            "\n",
            "\"No, I'm the best!\"\n",
            "\n",
            "\"Don't get it, girl. You're the best!\"\n",
            "\n",
            "\"I'm the best, I'm the best.\"\n",
            "\n",
            "\"I'm the best, I'm the best.\"\n",
            "\n",
            "\"I'm the best, girl. You're the best.\"\n",
            "\n",
            "\"No, I'm the best.\"\n",
            "\n",
            "\"Don't get it, girl. You're the best!\"\n",
            "\n",
            "\"I'm the best, girl. You're the best.\"\n",
            "\n",
            "\"I'm\n",
            "======================================== SAMPLE 6 ========================================\n",
            "There is a difference between a woman and a man, and the difference between a man and a woman is that a man has an erection and a woman has a vagina.\n",
            "\n",
            "- Sir\n",
            "\n",
            "Dear Sir,\n",
            "\n",
            "I have a friend who is a great mathematician and a great dancer. He says that his wife is the best dancer in the world, and that he loves her. He also said that she is the best dancer in the world. Is that a fair enough argument to make?\n",
            "\n",
            "- Sir\n",
            "\n",
            "Dear Sir,\n",
            "\n",
            "I am amazed that a man can love his wife so well. Why do you ask?\n",
            "\n",
            "- Sir\n",
            "\n",
            "Dear Sir,\n",
            "\n",
            "I have a really bad feeling about this subject. I think that I have lost the ability to concentrate. I am afraid that my brain has been compromised. I am afraid that my penis may be damaged. I am afraid that I may not be able to concentrate enough to be able to take care of\n",
            "======================================== SAMPLE 7 ========================================\n",
            "The latest from the CIA, and only a few weeks away from its release, this is the CIA's most recent report. The report is published online, and I was able to get it down to a few words.\n",
            "\n",
            "\"As soon as the CIA's director, John Brennan, was informed that the CIA had entered into a contract with a Russian intelligence agency to buy a weapons system that would be capable of delivering weapons of mass destruction to a vast area of the world.\n",
            "\n",
            "The deal was agreed upon by both parties and the first-in-line weapon was delivered on 23 July.\n",
            "\n",
            "There were no casualties, and no casualties, and no casualties, the Russian authorities said, except for three deaths in the process.\n",
            "\n",
            "\"The Russians are very disappointed.\n",
            "\n",
            "\"We have a lot of confidence as to the success of the Russian arms sales, and we are extremely confident that the Russian authorities will make an informed decision at the appropriate time.\n",
            "\n",
            "In the final\n",
            "======================================== SAMPLE 8 ========================================\n",
            ">What's the difference between the black and red one?\n",
            "\n",
            "\n",
            "A black and red one is not the same. A black one is a black thing.<|endoftext|>\"The only thing worth knowing about the Holocaust is that it was just a good experiment\"|\"Hitler did it. The only problem is he killed more people than he killed himself.\"|\"There are 3 types of Jews: the average Jews, the average Jews only, and the average Jew...<|endoftext|>A man walks into a bar and orders a beer!|The bartender says, \"Sir, you must be a good guy, you've got to do this. You should know that I can't drink your beer.\"\n",
            "\n",
            "The man says, \"I don't drink beer, I'm on drugs...\" and the bartender says, \"That's because I'm not a lawyer, I'm a bartender.\"<|endoftext|>My wife asked this question\n",
            "======================================== SAMPLE 9 ========================================\n",
            ">My girlfriend is an anarchist...|She's a lesbian.\n",
            "\n",
            "Me: I'm not an anarchist.\n",
            "\n",
            "Me: I don't think you should either!\n",
            "\n",
            "Me: No, I'm not an anarchist.\n",
            "\n",
            "Me: I'm not an anarchist.\n",
            "\n",
            "Me: I'm not an anarchist.\n",
            "\n",
            "Me: I don't think you should either!\n",
            "\n",
            "Me: I'm not an anarchist.\n",
            "\n",
            "Me: I'm not an anarchist.\n",
            "\n",
            "Me: I don't think you should either!\n",
            "\n",
            "Me: I don't think you should either!\n",
            "\n",
            "Me: I'm not an anarchist.\n",
            "\n",
            "Me: I'm not an anarchist.\n",
            "\n",
            "Me: I'm not an anarchist.\n",
            "\n",
            "Me: I'm not an anarchist.\n",
            "\n",
            "\n",
            "(I don't know what the fuck these words are about!)\n",
            "\n",
            "Me: I'm not an anarchist.\n",
            "\n",
            "Me: I'm not an anarchist.\n",
            "\n",
            "\n",
            "======================================== SAMPLE 10 ========================================\n",
            "<|endoftext|>I went to a party for the rich and famous. They had a huge crowd and everyone was very nice.|And the bartender was really nice towards me. He said that the rich ones had a great time and the famous ones were good. So I got a beer and a drink.\n",
            "\n",
            "When I got home, I asked the bartender, \"What is that?\" He said, \"It's a very small beer. It tastes like a beer.\"\n",
            "\n",
            "I said, \"That's a small beer, but it's a good one.\"\n",
            "\n",
            "The bartender said, \"What? What's that?\"\n",
            "\n",
            "I said, \"That's a very small beer. It tastes like a beer.\"<|endoftext|>What does a man say when he gets a new pair of shoes?|The first one is a great deal.<|endoftext|>What do you call a Mexican who is a virgin?|A virgin\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}